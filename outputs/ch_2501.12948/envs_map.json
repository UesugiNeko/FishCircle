[
    {
        "placeholder": "<PLACEHOLDER_ENV_1>",
        "env_name": "list",
        "content": "\\begin{list}{}\n         {\\setlength{\\leftmargin}{#1}}\n         \\item[]\n }\n {\\end{list}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_2>",
        "env_name": "array",
        "content": "\\begin{array}{ccc} #1 & \\longrightarrow & #2 \\\\ #3 & \\longmapsto & #4 \\end{array}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_3>",
        "env_name": "array",
        "content": "\\begin{array}{l} #1 \\end{array}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_4>",
        "env_name": "abstract",
        "content": "\\begin{abstract}\n\nWe introduce our first-generation reasoning models, \\dsro{} and \\dsri{}.\n\\dsro{}, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, \\dsro{} naturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce \\dsri{}, which incorporates multi-stage training and cold-start data before RL.\n\\dsri{} achieves performance comparable to OpenAI-o1-1217 on reasoning tasks.\nTo support the research community, we open-source \\dsro{}, \\dsri{}, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from \\dsri{} based on Qwen and Llama.\n\n\\end{abstract}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_5>",
        "env_name": "figure",
        "content": "\\begin{figure}[h]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{figures/dsr1_performance.pdf}\n<PLACEHOLDER_CAP_2>\n\\label{fig:dsv3_performance}\n\\end{figure}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_6>",
        "env_name": "spacing",
        "content": "\\begin{spacing}{0.9}\n\\tableofcontents\n\\end{spacing}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_7>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n    \\item\n    We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of \\dsro{}. \\dsro{} demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n    \\item\n    We introduce our pipeline to develop \\dsri{}. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models.\n\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_8>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n    \\item We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source \\dsri{}, as well as its API, will benefit the research community to distill better smaller models in the future.\n    \\item Using the reasoning data generated by \\dsri{}, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks.\n    DeepSeek-R1-Distill-Qwen-7B achieves 55.5\\% on AIME 2024, surpassing QwQ-32B-Preview. Additionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6\\% on AIME 2024, 94.3\\% on MATH-500, and 57.2\\% on LiveCodeBench. These results significantly outperform previous open-source models and are comparable to o1-mini.\n    We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_9>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n\n    \\item \\textbf{Reasoning tasks}:\n    (1)\n   \\dsri{} achieves a score of 79.8\\% Pass@1 on AIME 2024, slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3\\%, performing on par with OpenAI-o1-1217 and significantly outperforming other models.\n    (2)\n    On coding-related tasks, \\dsri{} demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3\\% human participants in the competition.\n    For engineering-related tasks,  \\dsri{} performs slightly better than DeepSeek-V3, which could help developers in real world tasks.\n\n    \\item \\textbf{Knowledge}:\nOn benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, \\dsri{} achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8\\% on MMLU, 84.0\\% on MMLU-Pro, and 71.5\\% on GPQA Diamond. While its performance is slightly below that of OpenAI-o1-1217 on these benchmarks, \\dsri{} surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, \\dsri{} outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses 4o on this benchmark.\n    \\item \\textbf{Others}: \\dsri{} also excels in a wide range of tasks, including creative writing, general question answering, editing, summarization, and more. It achieves an impressive length-controlled win-rate of 87.6\\% on AlpacaEval 2.0 and a win-rate of 92.3\\% on ArenaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries. Additionally, \\dsri{} demonstrates outstanding performance on tasks requiring long-context understanding, substantially outperforming DeepSeek-V3 on long-context benchmarks.\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_10>",
        "env_name": "equation",
        "content": "\\begin{equation}\n\\begin{split}\n    \\mathcal{J}_{GRPO}(\\theta) &= \\mathbb{E}{[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)]}  \\\\\n    & \\frac{1}{G}\\sum_{i=1}^G \\left( \\min \\left( \\frac{\\pi_\\theta(o_i |q)}{\\pi_{\\theta_{old}}(o_i |q)} A_i, \\text{clip} \\left( \\frac{\\pi_\\theta(o_i |q)}{\\pi_{\\theta_{old}}(o_i |q)}, 1 - \\epsilon, 1 + \\epsilon \\right)  A_i \\right) - \\beta \\mathbb{D}_{KL}\\left(\\pi_{\\theta} || \\pi_{ref}\\right)\\right) ,\n\\end{split}\n\\label{eq:GRPO-obj}\n\\end{equation}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_11>",
        "env_name": "equation",
        "content": "\\begin{equation}\n    \\mathbb{D}_{KL}\\left(\\pi_{\\theta} || \\pi_{ref}\\right) = \\frac{\\pi_{ref}(o_i|q)}{\\pi_{\\theta}(o_i|q)}- \\log\\frac{\\pi_{ref}(o_i|q)}{\\pi_{\\theta}(o_i|q)} - 1,\n\\end{equation}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_12>",
        "env_name": "equation",
        "content": "\\begin{equation}\n    A_i = \\frac{r_i - {\\mathrm mean(\\{r_1, r_2, \\cdots, r_G\\})}}{{\\mathrm std(\\{r_1, r_2, \\cdots, r_G\\})}}.\n\\end{equation}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_13>",
        "env_name": "table",
        "content": "\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{l}\n    \\toprule\n    A conversation between User and Assistant. The user asks a question, and the Assistant solves it. \\\\\n     The assistant first thinks about the reasoning process in the mind and then provides the user \\\\ with the answer.\n     The reasoning process and answer are enclosed within <think> </think> and \\\\<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> \\\\ <answer> answer here </answer>.\n     User: \\textcolor{red}{prompt}. Assistant: \\\\\n     \\bottomrule\n    \\end{tabular}\n    <PLACEHOLDER_CAP_3>\n    \\label{tab:r0_template}\n\\end{table}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_14>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n    \\item \\textbf{Accuracy rewards}: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.\n    \\item \\textbf{Format rewards}: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between `<think>' and `</think>' tags.\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_15>",
        "env_name": "table",
        "content": "\\begin{table}[t]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}l *{6}{c} @{}}\n    \\toprule\n    \\multirow{3}{*}{\\centering\\textbf{Model}} & \\multicolumn{2}{c}{\\multirow{2}{*}{\\textbf{AIME 2024}}} & \\multirow{2}{*}{\\textbf{MATH-500}} & \\textbf{GPQA} & \\textbf{LiveCode} & \\multirow{2}{*}{\\textbf{CodeForces}} \\\\\n     &  & &  & \\textbf{Diamond} & \\textbf{Bench} \\\\\n    \\cmidrule(lr){2-3}\n     & pass@1 & cons@64 & pass@1 &  pass@1 & pass@1 & rating \\\\\n    \\midrule\n    \\textbf{OpenAI-o1-mini} & 63.6 & 80.0 & 90.0  & 60.0 & 53.8 & 1820 \\\\\n    \\textbf{OpenAI-o1-0912} & 74.4  & 83.3  & 94.8  & 77.3 & 63.4 & 1843 \\\\\n    \\midrule\n    \\textbf{\\dsro{}} & 71.0 & 86.7 & 95.9 & 73.3 & 50.0 & 1444 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    <PLACEHOLDER_CAP_4>\n    \\label{tab:r1-zero}\n\\end{table}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_16>",
        "env_name": "figure",
        "content": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{figures/plot_aime_with_maj.png}\n    <PLACEHOLDER_CAP_5>\n    \\label{fig:zero-training-performance}\n\\end{figure}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_17>",
        "env_name": "figure",
        "content": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{figures/plot_length.png}\n    <PLACEHOLDER_CAP_6>\n    \\label{fig:zero-training-length}\n\\end{figure}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_18>",
        "env_name": "table",
        "content": "\\begin{table}[t]\n\\centering\n\\begin{tabular}{l}\n\\toprule\nQuestion: If \\(a > 1\\), then the sum of the real solutions of \\(\\sqrt{a - \\sqrt{a + x}} = x\\) is equal to \\\\\n\\midrule\nResponse: <think> \\\\\nTo solve the equation \\(\\sqrt{a - \\sqrt{a + x}} = x\\), let's start by squaring both $\\cdots$ \\\\\n$\\left(\\sqrt{a - \\sqrt{a + x}}\\right)^2 = x^2 \\implies a - \\sqrt{a + x} = x^2.$ \\\\\nRearrange to isolate the inner square root term: \\\\\n$(a - x^2)^2 = a + x \\implies a^2 - 2ax^2 + (x^2)^2 = a + x \\implies x^4 - 2ax^2 - x + (a^2 - a) = 0$ \\\\\n\\ldots \\\\\n\\textcolor{red}{Wait, wait. Wait. That's an aha moment I can flag here}. \\\\\nLet's reevaluate this step-by-step to identify if the correct sum can be $\\cdots$ \\\\\nWe started with the equation: \\\\\n$\\sqrt{a - \\sqrt{a + x}} = x$ \\\\\nFirst, let's square both sides: \\\\\n$a - \\sqrt{a + x} = x^2 \\implies \\sqrt{a + x} = a - x^2$ \\\\\nNext, I could square both sides again, treating the equation:\n$\\cdots$\n\\\\\n\\ldots \\\\\n\\hline\n\\end{tabular}\n<PLACEHOLDER_CAP_7>\n\\label{tab:aha_moment}\n\\end{table}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_19>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n    \\item\nReadability: A key limitation of \\dsro{} is that its content is often not suitable for reading. Responses may mix multiple languages or lack markdown formatting to highlight answers for users. In contrast, when creating cold-start data for \\dsri{}, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly. Here, we define the output format as |special\\_token|<reasoning\\_process>|special\\_token|<summary>, where the reasoning process is the CoT for the query, and the summary is used to summarize the reasoning results.\n\n \\item\nPotential: By carefully designing the pattern for cold-start data with human priors, we observe better performance against \\dsro{}. We believe the iterative training is a better way for reasoning models.\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_20>",
        "env_name": "table",
        "content": "\\begin{table}[h]\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{1.9pt}\n    \\begin{tabular}{@{}c l | c  c  c | c c |c c@{}}\n    \\toprule\n    & \\multirow{2}{*}{\\centering \\textbf{Benchmark {\\tiny (Metric)}}}  & \\textbf{Claude-3.5-}  & \\textbf{GPT-4o}& \\textbf{DeepSeek} & \\textbf{OpenAI} & \\textbf{OpenAI} & \\textbf{DeepSeek}\\\\\n    & & \\textbf{Sonnet-1022}  & \\textbf{0513} & \\textbf{V3} & \\textbf{o1-mini}& \\textbf{o1-1217} &\\textbf{R1} \\\\\n    \\midrule\n    & Architecture &-&- & MoE &-&-& MoE \\\\\n    & \\# Activated Params& -&-& 37B&-&- & 37B \\\\\n    & \\# Total Params &-&-& 671B&-&- & 671B \\\\\n    \\midrule\n    \\multirow{10}{*}{English}& MMLU {\\tiny (Pass@1)} & 88.3&87.2 & 88.5 & 85.2 & \\textbf{91.8} & 90.8\\\\\n     & MMLU-Redux {\\tiny (EM)}& 88.9& 88.0 & 89.1 & 86.7&- & \\textbf{92.9} \\\\\n    & MMLU-Pro {\\tiny (EM)}  & 78.0 & 72.6 & 75.9 & 80.3 &-& \\textbf{84.0} \\\\\n    & DROP {\\tiny (3-shot F1)}  & 88.3 & 83.7 & 91.6 & 83.9 & 90.2 & \\textbf{92.2}\\\\\n    & IF-Eval {\\tiny (Prompt Strict)}  & \\textbf{86.5} & 84.3 & 86.1 & 84.8&- & 83.3 \\\\\n    & GPQA Diamond {\\tiny (Pass@1)}& 65.0 & 49.9 & 59.1 & 60.0 & \\textbf{75.7} & 71.5&  \\\\\n    & SimpleQA {\\tiny (Correct)} & 28.4 & 38.2& 24.9 & 7.0 & \\textbf{47.0} & 30.1 \\\\\n     & FRAMES {\\tiny (Acc.)}  & 72.5 & 80.5 & 73.3 & 76.9 & -&\\textbf{82.5}\\\\\n      & AlpacaEval2.0 {\\tiny (LC-winrate)}  & 52.0 &  51.1 & 70.0 & 57.8 & - & \\textbf{87.6}\\\\\n       & ArenaHard {\\tiny (GPT-4-1106)}  & 85.2 & 80.4 & 85.5 & 92.0 & - & \\textbf{92.3}\\\\\n    \\midrule\n    \\multirow{4}{*}{Code} & LiveCodeBench {\\tiny (Pass@1-COT)} & 38.9 & 32.9 & 36.2 & 53.8 & 63.4 & \\textbf{65.9} \\\\\n    & Codeforces {\\tiny (Percentile)}& 20.3 & 23.6 & 58.7 & 93.4 & \\textbf{96.6} & 96.3 \\\\\n    & Codeforces {\\tiny (Rating)}& 717 & 759 & 1134 & 1820 & \\textbf{2061} & 2029 \\\\\n    & SWE Verified {\\tiny (Resolved)} & \\textbf{50.8}&38.8&42.0 & 41.6 & 48.9 & 49.2\\\\\n    & Aider-Polyglot {\\tiny (Acc.)} & 45.3&16.0& 49.6 & 32.9 & \\textbf{61.7}&53.3\\\\\n    \\midrule\n    \\multirow{3}{*}{Math} & AIME 2024 {\\tiny (Pass@1)}  & 16.0 & 9.3 & 39.2 & 63.6 & 79.2 & \\textbf{79.8} \\\\\n    & MATH-500 {\\tiny (Pass@1)} &78.3 & 74.6&90.2 & 90.0 & 96.4 & \\textbf{97.3} \\\\\n    & CNMO 2024 {\\tiny (Pass@1)} & 13.1 & 10.8 &43.2 & 67.6 & - & \\textbf{78.8} \\\\\n    \\midrule\n    \\multirow{3}{*}{Chinese} & CLUEWSC {\\tiny (EM)}&  85.4 & 87.9 & 90.9 & 89.9 & - &\\textbf{92.8}\\\\\n    & C-Eval {\\tiny (EM)} & 76.7 & 76.0 & 86.5 & 68.9 & - & \\textbf{91.8}\\\\\n     & C-SimpleQA {\\tiny (Correct)}  & 55.4 & 58.7 & \\textbf{68.0} & 40.3 & -& 63.7 \\\\\n    \\bottomrule\n    \\end{tabular}\n    <PLACEHOLDER_CAP_8>\n    \\label{tab:main}\n\\end{table}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_21>",
        "env_name": "table",
        "content": "\\begin{table}[h]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}l *{6}{c} @{}}\n    \\toprule\n    \\multirow{3}{*}{\\centering\\textbf{Model}} & \\multicolumn{2}{c}{\\multirow{2}{*}{\\textbf{AIME 2024}}} & \\multirow{2}{*}{\\textbf{MATH-500}} & \\textbf{GPQA} & \\textbf{LiveCode} & \\multirow{2}{*}{\\textbf{CodeForces}} \\\\\n    &  &  &  & \\textbf{Diamond} & \\textbf{Bench} \\\\\n    \\cmidrule(lr){2-3}\n     & pass@1 & cons@64 & pass@1 & pass@1 & pass@1 & rating \\\\\n    \\midrule\n    \\textbf{GPT-4o-0513} & 9.3 & 13.4 & 74.6  & 49.9 & 32.9 &  759\\\\\n    \\textbf{Claude-3.5-Sonnet-1022} & 16.0 & 26.7 & 78.3  & 65.0 & 38.9 &  717\\\\\n    \\textbf{OpenAI-o1-mini} & 63.6 & 80.0 & 90.0 &  60.0 & 53.8 &  \\textbf{1820}\\\\\n    \\textbf{QwQ-32B-Preview} & 50.0 & 60.0 & 90.6 & 54.5 & 41.9 &  1316 \\\\\n    \\midrule\n    \\textbf{DeepSeek-R1-Distill-Qwen-1.5B} & 28.9 & 52.7 & 83.9 & 33.8 & 16.9 & 954 \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-7B} & 55.5 & 83.3 & 92.8 & 49.1 & 37.6 & 1189 \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-14B} & 69.7 & 80.0 & 93.9 &  59.1 & 53.1 & 1481 \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-32B} & \\textbf{72.6} & {83.3} & {94.3} & {62.1} & {57.2} & 1691 \\\\\n    \\textbf{DeepSeek-R1-Distill-Llama-8B} & 50.4 & 80.0 & 89.1 & 49.0 & 39.6 & 1205 \\\\\n    \\textbf{DeepSeek-R1-Distill-Llama-70B} & 70.0 & \\textbf{86.7} & \\textbf{94.5} & \\textbf{65.2} & \\textbf{57.5} & 1633 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    <PLACEHOLDER_CAP_9>\n    \\label{tab:distill}\n\\end{table}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_22>",
        "env_name": "table",
        "content": "\\begin{table}[h]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{@{}l *{6}{c} @{}}\n    \\toprule\n    \\multirow{3}{*}{\\centering\\textbf{Model}} & \\multicolumn{2}{c}{\\textbf{AIME 2024}} & \\textbf{MATH-500} & \\textbf{GPQA Diamond} & \\textbf{LiveCodeBench}  \\\\\n\n    \\cmidrule(lr){2-3}\n     & pass@1 & cons@64 & pass@1& pass@1 & pass@1 \\\\\n    \\midrule\n    \\textbf{QwQ-32B-Preview} & 50.0 & 60.0 & 90.6 & 54.5 & 41.9  \\\\\n    \\textbf{DeepSeek-R1-Zero-Qwen-32B} & 47.0 & 60.0 & 91.6  & 55.0 & 40.2  \\\\\n    \\textbf{DeepSeek-R1-Distill-Qwen-32B} & \\bf{72.6} & \\bf{83.3} & \\bf{94.3}  & \\bf{62.1} & \\bf{57.2}\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    <PLACEHOLDER_CAP_10>\n    \\label{tab:distill_vs_rl}\n\\end{table}",
        "trans_content": "",
        "need_trans": false
    },
    {
        "placeholder": "<PLACEHOLDER_ENV_23>",
        "env_name": "itemize",
        "content": "\\begin{itemize}[topsep=0pt]\n    \\item \\textbf{General Capability:}\n  Currently, the capabilities of \\dsri{} fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Moving forward, we plan to explore how long CoT can be leveraged to enhance tasks in these fields.\n    \\item \\textbf{Language Mixing:}\n\\dsri{} is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, \\dsri{} might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.\n \\item \\textbf{Prompting Engineering:} When evaluating \\dsri{}, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.\n\\item  \\textbf{Software Engineering Tasks:}\nDue to the long evaluation times, which impact the efficiency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing rejection sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency.\n\n\\end{itemize}",
        "trans_content": "",
        "need_trans": true
    }
]