[
    {
        "placeholder": "<PLACEHOLDER_CAP_1>",
        "cap_type": "title",
        "content": "\\title{\\centering DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}",
        "trans_content": "\\title{\\centering DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_2>",
        "cap_type": "caption",
        "content": "\\caption{\n    \\centering\n    Benchmark performance of \\dsri{}.\n}",
        "trans_content": "\\caption{\n    \\centering\n    Benchmark performance of \\dsri{}.\n}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_3>",
        "cap_type": "caption",
        "content": "\\caption{Template for \\dsro{}. \\textcolor{red}{prompt} will be replaced with the specific reasoning question during training.}",
        "trans_content": "\\caption{Template for \\dsro{}. \\textcolor{red}{prompt} will be replaced with the specific reasoning question during training.}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_4>",
        "cap_type": "caption",
        "content": "\\caption{Comparison of \\dsro{} and OpenAI o1 models on reasoning-related benchmarks.}",
        "trans_content": "\\caption{Comparison of \\dsro{} and OpenAI o1 models on reasoning-related benchmarks.}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_5>",
        "cap_type": "caption",
        "content": "\\caption{AIME accuracy of \\dsro{} during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.}",
        "trans_content": "\\caption{AIME accuracy of \\dsro{} during training. For each question, we sample 16 responses and calculate the overall average accuracy to ensure a stable evaluation.}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_6>",
        "cap_type": "caption",
        "content": "\\caption{The average response length of \\dsro{} on the training set during the RL process. \\dsro{} naturally learns to solve reasoning tasks with more thinking time.}",
        "trans_content": "\\caption{The average response length of \\dsro{} on the training set during the RL process. \\dsro{} naturally learns to solve reasoning tasks with more thinking time.}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_7>",
        "cap_type": "caption",
        "content": "\\caption{An interesting ``aha moment'' of an intermediate version of \\dsro{}. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.}",
        "trans_content": "\\caption{An interesting ``aha moment'' of an intermediate version of \\dsro{}. The model learns to rethink using an anthropomorphic tone. This is also an aha moment for us, allowing us to witness the power and beauty of reinforcement learning.}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_8>",
        "cap_type": "caption",
        "content": "\\caption{ Comparison between \\dsri{} and other representative models.\n    }",
        "trans_content": "\\caption{ Comparison between \\dsri{} and other representative models.\n    }"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_9>",
        "cap_type": "caption",
        "content": "\\caption{Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.}",
        "trans_content": "\\caption{Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks.}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_10>",
        "cap_type": "caption",
        "content": "\\caption{\\centering Comparison of distilled and RL Models on Reasoning-Related Benchmarks.}",
        "trans_content": "\\caption{\\centering Comparison of distilled and RL Models on Reasoning-Related Benchmarks.}"
    }
]