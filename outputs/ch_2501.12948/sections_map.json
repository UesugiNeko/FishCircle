[
    {
        "section": "-1",
        "content": "\\documentclass[11pt, a4paper, logo, copyright, nonumbering]{deepseek}\n\\usepackage[authoryear, sort&compress, round]{natbib}\n\\usepackage{dblfloatfix}\n\\usepackage{ulem}\n\\usepackage{caption}\n\\usepackage{dramatist}\n\\usepackage{xspace}\n\\usepackage{pifont}\n\\usepackage{multirow}\n\\usepackage{tcolorbox}\n\\usepackage{xltabular}\n\\usepackage{longtable}\n\\usepackage{hyperref}\n\\interfootnotelinepenalty=10000\n\n\\usepackage{amsfonts}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{lineno}\n\\usepackage{multirow}\n\\usepackage{adjustbox}\n\n\\usepackage[bottom]{footmisc}\n\n\\usepackage{CJKutf8}\n\\usepackage{subfigure}\n\\usepackage{setspace}\n\n\\usepackage{dsfont}\n\\usepackage{array}\n\\usepackage{tabularx}\n\\usepackage{subfigure}\n\\usepackage{xcolor}\n\\usepackage{tabularx}\n\\usepackage{booktabs}\n\n\\usepackage{lipsum}\n\\usepackage{multicol}\n\n\\makeatletter\n\\def\\@BTrule[#1]{\n  \\ifx\\longtable\\undefined\n    \\let\\@BTswitch\\@BTnormal\n  \\else\\ifx\\hline\\LT@hline\n    \\nobreak\n    \\let\\@BTswitch\\@BLTrule\n  \\else\n     \\let\\@BTswitch\\@BTnormal\n  \\fi\\fi\n  \\global\\@thisrulewidth=#1\\relax\n  \\ifnum\\@thisruleclass=\\tw@\\vskip\\@aboverulesep\\else\n  \\ifnum\\@lastruleclass=\\z@\\vskip\\@aboverulesep\\else\n  \\ifnum\\@lastruleclass=\\@ne\\vskip\\doublerulesep\\fi\\fi\\fi\n  \\@BTswitch}\n\\makeatother\n\n\\addto\\extrasenglish{\n    <PLACEHOLDER_NEWCOMMAND_0>\n    <PLACEHOLDER_NEWCOMMAND_1>\n    <PLACEHOLDER_NEWCOMMAND_2>\n    <PLACEHOLDER_NEWCOMMAND_3>\n    <PLACEHOLDER_NEWCOMMAND_4>\n    <PLACEHOLDER_NEWCOMMAND_5>\n    <PLACEHOLDER_NEWCOMMAND_6>\n}\n\n\\newenvironment{indentchunk}[1]\n {<PLACEHOLDER_ENV_1>}\n\n\\bibliographystyle{abbrvnat}\n\n\\reportnumber{001}\n\n<PLACEHOLDER_NEWCOMMAND_7>\n\n<PLACEHOLDER_CAP_1>\n\n\\author[*]{\nDeepSeek-AI\n\\\\\n\\small\n\\texttt{research@deepseek.com}\n}\n<PLACEHOLDER_commands.tex_begin><PLACEHOLDER_NEWCOMMAND_8>\n<PLACEHOLDER_NEWCOMMAND_9>\n<PLACEHOLDER_NEWCOMMAND_10>\n<PLACEHOLDER_NEWCOMMAND_11>\n<PLACEHOLDER_NEWCOMMAND_12>\n<PLACEHOLDER_NEWCOMMAND_13>\n<PLACEHOLDER_NEWCOMMAND_14>\n<PLACEHOLDER_NEWCOMMAND_15>\n<PLACEHOLDER_NEWCOMMAND_16>\n<PLACEHOLDER_NEWCOMMAND_17>\n<PLACEHOLDER_NEWCOMMAND_18>\n<PLACEHOLDER_NEWCOMMAND_19>\n<PLACEHOLDER_NEWCOMMAND_20>\n<PLACEHOLDER_NEWCOMMAND_21>\n<PLACEHOLDER_NEWCOMMAND_22>\n<PLACEHOLDER_NEWCOMMAND_23>\n<PLACEHOLDER_NEWCOMMAND_24>\n<PLACEHOLDER_NEWCOMMAND_25>\n<PLACEHOLDER_NEWCOMMAND_26>\n<PLACEHOLDER_NEWCOMMAND_27>\n<PLACEHOLDER_NEWCOMMAND_28>\n<PLACEHOLDER_NEWCOMMAND_29>\n<PLACEHOLDER_NEWCOMMAND_30>\n<PLACEHOLDER_NEWCOMMAND_31>\n<PLACEHOLDER_NEWCOMMAND_32>\n<PLACEHOLDER_NEWCOMMAND_33>\n<PLACEHOLDER_NEWCOMMAND_34>\n<PLACEHOLDER_NEWCOMMAND_35>\n<PLACEHOLDER_NEWCOMMAND_36>\n<PLACEHOLDER_NEWCOMMAND_37>\n<PLACEHOLDER_NEWCOMMAND_38>\n<PLACEHOLDER_NEWCOMMAND_39>\n<PLACEHOLDER_NEWCOMMAND_40>\n<PLACEHOLDER_NEWCOMMAND_41>\n<PLACEHOLDER_NEWCOMMAND_42>\n<PLACEHOLDER_NEWCOMMAND_43>\n<PLACEHOLDER_NEWCOMMAND_44>\n\n<PLACEHOLDER_NEWCOMMAND_45>\n\n<PLACEHOLDER_NEWCOMMAND_46>\n\n<PLACEHOLDER_NEWCOMMAND_47>\n<PLACEHOLDER_NEWCOMMAND_48>\n<PLACEHOLDER_NEWCOMMAND_49>\n<PLACEHOLDER_NEWCOMMAND_50>\n<PLACEHOLDER_NEWCOMMAND_51>\n<PLACEHOLDER_NEWCOMMAND_52>\n<PLACEHOLDER_NEWCOMMAND_53>\n<PLACEHOLDER_NEWCOMMAND_54>\n<PLACEHOLDER_NEWCOMMAND_55>\n<PLACEHOLDER_NEWCOMMAND_56>\n<PLACEHOLDER_NEWCOMMAND_57>\n<PLACEHOLDER_NEWCOMMAND_58>\n<PLACEHOLDER_NEWCOMMAND_59>\n<PLACEHOLDER_NEWCOMMAND_60>\n<PLACEHOLDER_NEWCOMMAND_61>\n<PLACEHOLDER_NEWCOMMAND_62>\n<PLACEHOLDER_NEWCOMMAND_63>\n<PLACEHOLDER_NEWCOMMAND_64>\n<PLACEHOLDER_NEWCOMMAND_65>\n<PLACEHOLDER_NEWCOMMAND_66>\n<PLACEHOLDER_NEWCOMMAND_67>\n<PLACEHOLDER_NEWCOMMAND_68>\n<PLACEHOLDER_NEWCOMMAND_69>\n<PLACEHOLDER_NEWCOMMAND_70>\n<PLACEHOLDER_NEWCOMMAND_71>\n<PLACEHOLDER_NEWCOMMAND_72>\n<PLACEHOLDER_NEWCOMMAND_73>\n<PLACEHOLDER_NEWCOMMAND_74>\n<PLACEHOLDER_NEWCOMMAND_75>\n<PLACEHOLDER_NEWCOMMAND_76>\n<PLACEHOLDER_NEWCOMMAND_77>\n<PLACEHOLDER_NEWCOMMAND_78>\n<PLACEHOLDER_NEWCOMMAND_79>\n<PLACEHOLDER_NEWCOMMAND_80>\n<PLACEHOLDER_NEWCOMMAND_81>\n<PLACEHOLDER_NEWCOMMAND_82>\n<PLACEHOLDER_NEWCOMMAND_83>\n<PLACEHOLDER_NEWCOMMAND_84>\n<PLACEHOLDER_NEWCOMMAND_85>\n<PLACEHOLDER_NEWCOMMAND_86>\n<PLACEHOLDER_NEWCOMMAND_87>\n<PLACEHOLDER_NEWCOMMAND_88>\n<PLACEHOLDER_NEWCOMMAND_89>\n\n<PLACEHOLDER_NEWCOMMAND_90>\n<PLACEHOLDER_NEWCOMMAND_91>\n<PLACEHOLDER_NEWCOMMAND_92>\n<PLACEHOLDER_NEWCOMMAND_93>\n<PLACEHOLDER_NEWCOMMAND_94>\n<PLACEHOLDER_NEWCOMMAND_95>\n<PLACEHOLDER_NEWCOMMAND_96>\n<PLACEHOLDER_NEWCOMMAND_97>\n<PLACEHOLDER_NEWCOMMAND_98>\n<PLACEHOLDER_NEWCOMMAND_99>\n<PLACEHOLDER_NEWCOMMAND_100>\n<PLACEHOLDER_NEWCOMMAND_101>\n<PLACEHOLDER_NEWCOMMAND_102>\n\n<PLACEHOLDER_NEWCOMMAND_103>\n<PLACEHOLDER_NEWCOMMAND_104>\n<PLACEHOLDER_NEWCOMMAND_105>\n<PLACEHOLDER_NEWCOMMAND_106>\n<PLACEHOLDER_NEWCOMMAND_107>\n<PLACEHOLDER_NEWCOMMAND_108>\n<PLACEHOLDER_NEWCOMMAND_109>\n<PLACEHOLDER_NEWCOMMAND_110>\n<PLACEHOLDER_NEWCOMMAND_111>\n<PLACEHOLDER_NEWCOMMAND_112>\n\n<PLACEHOLDER_NEWCOMMAND_113>\n\\let\\Re\\relax\n\n\\DeclareMathOperator{\\Re}{\\Rr e}\n\\DeclareMathOperator{\\Imag}{\\Ii m}\n\\DeclareMathOperator{\\Ker}{Ker}\n\\DeclareMathOperator{\\Hom}{Hom}\n\\DeclareMathOperator{\\End}{End}\n\\DeclareMathOperator{\\tr}{tr}\n\\DeclareMathOperator{\\Tr}{Tr}\n\\DeclareMathOperator{\\Supp}{Supp}\n\\DeclareMathOperator{\\Sign}{Sign}\n\\let\\Im\\relax\n\\DeclareMathOperator{\\Im}{Im}\n\\DeclareMathOperator{\\Corr}{Corr}\n\\DeclareMathOperator{\\sign}{sign}\n\\DeclareMathOperator{\\supp}{supp}\n\n\\DeclareMathOperator{\\cas}{cas}\n\\DeclareMathOperator{\\sinc}{sinc}\n\\DeclareMathOperator{\\cotan}{cotan}\n\\DeclareMathOperator{\\Card}{Card}\n\\DeclareMathOperator{\\GCD}{GCD}\n\\DeclareMathOperator{\\grad}{grad}\n\\DeclareMathOperator{\\Diag}{Diag}\n\\DeclareMathOperator{\\rank}{rank}\n\\DeclareMathOperator{\\conv}{conv}\n\\DeclareMathOperator{\\interop}{int}\n\n<PLACEHOLDER_NEWCOMMAND_114>\n\n<PLACEHOLDER_NEWCOMMAND_115>\n<PLACEHOLDER_NEWCOMMAND_116>\n<PLACEHOLDER_NEWCOMMAND_117>\n<PLACEHOLDER_NEWCOMMAND_118>\n<PLACEHOLDER_NEWCOMMAND_119>\n<PLACEHOLDER_NEWCOMMAND_120>\n<PLACEHOLDER_NEWCOMMAND_121>\n<PLACEHOLDER_NEWCOMMAND_122>\n<PLACEHOLDER_NEWCOMMAND_123>\n<PLACEHOLDER_NEWCOMMAND_124>\n<PLACEHOLDER_NEWCOMMAND_125>\n<PLACEHOLDER_NEWCOMMAND_126>\n<PLACEHOLDER_NEWCOMMAND_127>\n<PLACEHOLDER_NEWCOMMAND_128>\n<PLACEHOLDER_NEWCOMMAND_129>\n<PLACEHOLDER_NEWCOMMAND_130>\n<PLACEHOLDER_NEWCOMMAND_131>\n<PLACEHOLDER_NEWCOMMAND_132>\n<PLACEHOLDER_NEWCOMMAND_133>\n\n<PLACEHOLDER_NEWCOMMAND_134>\n<PLACEHOLDER_NEWCOMMAND_135>\n<PLACEHOLDER_NEWCOMMAND_136>\n<PLACEHOLDER_NEWCOMMAND_137>\n<PLACEHOLDER_NEWCOMMAND_138>\n<PLACEHOLDER_NEWCOMMAND_139>\n<PLACEHOLDER_NEWCOMMAND_140>\n<PLACEHOLDER_NEWCOMMAND_141>\n<PLACEHOLDER_NEWCOMMAND_142>\n\n<PLACEHOLDER_NEWCOMMAND_143>\n<PLACEHOLDER_NEWCOMMAND_144>\n<PLACEHOLDER_NEWCOMMAND_145>\n<PLACEHOLDER_NEWCOMMAND_146>\n<PLACEHOLDER_NEWCOMMAND_147>\n<PLACEHOLDER_NEWCOMMAND_148>\n<PLACEHOLDER_NEWCOMMAND_149>\n<PLACEHOLDER_NEWCOMMAND_150>\n<PLACEHOLDER_NEWCOMMAND_151>\n<PLACEHOLDER_NEWCOMMAND_152>\n<PLACEHOLDER_NEWCOMMAND_153>\n\\DeclareMathOperator{\\diverg}{div}\n\\DeclareMathOperator{\\Prox}{Prox}\n<PLACEHOLDER_NEWCOMMAND_154>\n<PLACEHOLDER_NEWCOMMAND_155>\n<PLACEHOLDER_NEWCOMMAND_156>\n<PLACEHOLDER_NEWCOMMAND_157>\n<PLACEHOLDER_NEWCOMMAND_158>\n<PLACEHOLDER_NEWCOMMAND_159>\n<PLACEHOLDER_NEWCOMMAND_160>\n\n\\newcommand{\\func}[4]{ {\\left\\{  <PLACEHOLDER_ENV_2>  \\right.} }\n<PLACEHOLDER_NEWCOMMAND_161>\n<PLACEHOLDER_NEWCOMMAND_162>\n\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\n<PLACEHOLDER_NEWCOMMAND_163>\n<PLACEHOLDER_NEWCOMMAND_164>\n\\newcommand{\\choice}[1]{\n\t\\left\\{\n\t\t<PLACEHOLDER_ENV_3>\n\t\\right. }\n<PLACEHOLDER_NEWCOMMAND_165>\n<PLACEHOLDER_NEWCOMMAND_166>\n\n<PLACEHOLDER_NEWCOMMAND_167>\n<PLACEHOLDER_NEWCOMMAND_168>\n<PLACEHOLDER_NEWCOMMAND_169>\n<PLACEHOLDER_NEWCOMMAND_170>\n<PLACEHOLDER_NEWCOMMAND_171>\n<PLACEHOLDER_NEWCOMMAND_172>\n<PLACEHOLDER_NEWCOMMAND_173>\n<PLACEHOLDER_NEWCOMMAND_174>\n<PLACEHOLDER_NEWCOMMAND_175>\n<PLACEHOLDER_NEWCOMMAND_176>\n<PLACEHOLDER_NEWCOMMAND_177>\n<PLACEHOLDER_NEWCOMMAND_178>\n<PLACEHOLDER_NEWCOMMAND_179>\n<PLACEHOLDER_NEWCOMMAND_180>\n<PLACEHOLDER_NEWCOMMAND_181>\n<PLACEHOLDER_NEWCOMMAND_182>\n<PLACEHOLDER_NEWCOMMAND_183>\n<PLACEHOLDER_NEWCOMMAND_184>\n<PLACEHOLDER_NEWCOMMAND_185>\n<PLACEHOLDER_NEWCOMMAND_186>\n\n\\newlength{\\restsubwidth}\n\\newlength{\\restsubheight}\n\\newlength{\\restsubmoreheight}\n\\setlength{\\restsubmoreheight}{4pt}\n<PLACEHOLDER_NEWCOMMAND_187><PLACEHOLDER_commands.tex_end>\n\n<PLACEHOLDER_NEWCOMMAND_188>\n<PLACEHOLDER_NEWCOMMAND_189>\n<PLACEHOLDER_NEWCOMMAND_190>\n<PLACEHOLDER_NEWCOMMAND_191>\n<PLACEHOLDER_NEWCOMMAND_192>\n\n<PLACEHOLDER_ENV_4>",
        "trans_content": "\\documentclass[11pt, a4paper, logo, copyright, nonumbering]{deepseek}\n\\usepackage[authoryear, sort&compress, round]{natbib}\n\\usepackage{dblfloatfix}\n\\usepackage{ulem}\n\\usepackage{caption}\n\\usepackage{dramatist}\n\\usepackage{xspace}\n\\usepackage{pifont}\n\\usepackage{multirow}\n\\usepackage{tcolorbox}\n\\usepackage{xltabular}\n\\usepackage{longtable}\n\\usepackage{hyperref}\n\\interfootnotelinepenalty=10000\n\n\\usepackage{amsfonts}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{lineno}\n\\usepackage{multirow}\n\\usepackage{adjustbox}\n\n\\usepackage[bottom]{footmisc}\n\n\\usepackage{CJKutf8}\n\\usepackage{subfigure}\n\\usepackage{setspace}\n\n\\usepackage{dsfont}\n\\usepackage{array}\n\\usepackage{tabularx}\n\\usepackage{subfigure}\n\\usepackage{xcolor}\n\\usepackage{tabularx}\n\\usepackage{booktabs}\n\n\\usepackage{lipsum}\n\\usepackage{multicol}\n\n\\makeatletter\n\\def\\@BTrule[#1]{\n  \\ifx\\longtable\\undefined\n    \\let\\@BTswitch\\@BTnormal\n  \\else\\ifx\\hline\\LT@hline\n    \\nobreak\n    \\let\\@BTswitch\\@BLTrule\n  \\else\n     \\let\\@BTswitch\\@BTnormal\n  \\fi\\fi\n  \\global\\@thisrulewidth=#1\\relax\n  \\ifnum\\@thisruleclass=\\tw@\\vskip\\@aboverulesep\\else\n  \\ifnum\\@lastruleclass=\\z@\\vskip\\@aboverulesep\\else\n  \\ifnum\\@lastruleclass=\\@ne\\vskip\\doublerulesep\\fi\\fi\\fi\n  \\@BTswitch}\n\\makeatother\n\n\\addto\\extrasenglish{\n    <PLACEHOLDER_NEWCOMMAND_0>\n    <PLACEHOLDER_NEWCOMMAND_1>\n    <PLACEHOLDER_NEWCOMMAND_2>\n    <PLACEHOLDER_NEWCOMMAND_3>\n    <PLACEHOLDER_NEWCOMMAND_4>\n    <PLACEHOLDER_NEWCOMMAND_5>\n    <PLACEHOLDER_NEWCOMMAND_6>\n}\n\n\\newenvironment{indentchunk}[1]\n {<PLACEHOLDER_ENV_1>}\n\n\\bibliographystyle{abbrvnat}\n\n\\reportnumber{001}\n\n<PLACEHOLDER_NEWCOMMAND_7>\n\n<PLACEHOLDER_CAP_1>\n\n\\author[*]{\nDeepSeek-AI\n\\\\\n\\small\n\\texttt{research@deepseek.com}\n}\n<PLACEHOLDER_commands.tex_begin><PLACEHOLDER_NEWCOMMAND_8>\n<PLACEHOLDER_NEWCOMMAND_9>\n<PLACEHOLDER_NEWCOMMAND_10>\n<PLACEHOLDER_NEWCOMMAND_11>\n<PLACEHOLDER_NEWCOMMAND_12>\n<PLACEHOLDER_NEWCOMMAND_13>\n<PLACEHOLDER_NEWCOMMAND_14>\n<PLACEHOLDER_NEWCOMMAND_15>\n<PLACEHOLDER_NEWCOMMAND_16>\n<PLACEHOLDER_NEWCOMMAND_17>\n<PLACEHOLDER_NEWCOMMAND_18>\n<PLACEHOLDER_NEWCOMMAND_19>\n<PLACEHOLDER_NEWCOMMAND_20>\n<PLACEHOLDER_NEWCOMMAND_21>\n<PLACEHOLDER_NEWCOMMAND_22>\n<PLACEHOLDER_NEWCOMMAND_23>\n<PLACEHOLDER_NEWCOMMAND_24>\n<PLACEHOLDER_NEWCOMMAND_25>\n<PLACEHOLDER_NEWCOMMAND_26>\n<PLACEHOLDER_NEWCOMMAND_27>\n<PLACEHOLDER_NEWCOMMAND_28>\n<PLACEHOLDER_NEWCOMMAND_29>\n<PLACEHOLDER_NEWCOMMAND_30>\n<PLACEHOLDER_NEWCOMMAND_31>\n<PLACEHOLDER_NEWCOMMAND_32>\n<PLACEHOLDER_NEWCOMMAND_33>\n<PLACEHOLDER_NEWCOMMAND_34>\n<PLACEHOLDER_NEWCOMMAND_35>\n<PLACEHOLDER_NEWCOMMAND_36>\n<PLACEHOLDER_NEWCOMMAND_37>\n<PLACEHOLDER_NEWCOMMAND_38>\n<PLACEHOLDER_NEWCOMMAND_39>\n<PLACEHOLDER_NEWCOMMAND_40>\n<PLACEHOLDER_NEWCOMMAND_41>\n<PLACEHOLDER_NEWCOMMAND_42>\n<PLACEHOLDER_NEWCOMMAND_43>\n<PLACEHOLDER_NEWCOMMAND_44>\n\n<PLACEHOLDER_NEWCOMMAND_45>\n\n<PLACEHOLDER_NEWCOMMAND_46>\n\n<PLACEHOLDER_NEWCOMMAND_47>\n<PLACEHOLDER_NEWCOMMAND_48>\n<PLACEHOLDER_NEWCOMMAND_49>\n<PLACEHOLDER_NEWCOMMAND_50>\n<PLACEHOLDER_NEWCOMMAND_51>\n<PLACEHOLDER_NEWCOMMAND_52>\n<PLACEHOLDER_NEWCOMMAND_53>\n<PLACEHOLDER_NEWCOMMAND_54>\n<PLACEHOLDER_NEWCOMMAND_55>\n<PLACEHOLDER_NEWCOMMAND_56>\n<PLACEHOLDER_NEWCOMMAND_57>\n<PLACEHOLDER_NEWCOMMAND_58>\n<PLACEHOLDER_NEWCOMMAND_59>\n<PLACEHOLDER_NEWCOMMAND_60>\n<PLACEHOLDER_NEWCOMMAND_61>\n<PLACEHOLDER_NEWCOMMAND_62>\n<PLACEHOLDER_NEWCOMMAND_63>\n<PLACEHOLDER_NEWCOMMAND_64>\n<PLACEHOLDER_NEWCOMMAND_65>\n<PLACEHOLDER_NEWCOMMAND_66>\n<PLACEHOLDER_NEWCOMMAND_67>\n<PLACEHOLDER_NEWCOMMAND_68>\n<PLACEHOLDER_NEWCOMMAND_69>\n<PLACEHOLDER_NEWCOMMAND_70>\n<PLACEHOLDER_NEWCOMMAND_71>\n<PLACEHOLDER_NEWCOMMAND_72>\n<PLACEHOLDER_NEWCOMMAND_73>\n<PLACEHOLDER_NEWCOMMAND_74>\n<PLACEHOLDER_NEWCOMMAND_75>\n<PLACEHOLDER_NEWCOMMAND_76>\n<PLACEHOLDER_NEWCOMMAND_77>\n<PLACEHOLDER_NEWCOMMAND_78>\n<PLACEHOLDER_NEWCOMMAND_79>\n<PLACEHOLDER_NEWCOMMAND_80>\n<PLACEHOLDER_NEWCOMMAND_81>\n<PLACEHOLDER_NEWCOMMAND_82>\n<PLACEHOLDER_NEWCOMMAND_83>\n<PLACEHOLDER_NEWCOMMAND_84>\n<PLACEHOLDER_NEWCOMMAND_85>\n<PLACEHOLDER_NEWCOMMAND_86>\n<PLACEHOLDER_NEWCOMMAND_87>\n<PLACEHOLDER_NEWCOMMAND_88>\n<PLACEHOLDER_NEWCOMMAND_89>\n\n<PLACEHOLDER_NEWCOMMAND_90>\n<PLACEHOLDER_NEWCOMMAND_91>\n<PLACEHOLDER_NEWCOMMAND_92>\n<PLACEHOLDER_NEWCOMMAND_93>\n<PLACEHOLDER_NEWCOMMAND_94>\n<PLACEHOLDER_NEWCOMMAND_95>\n<PLACEHOLDER_NEWCOMMAND_96>\n<PLACEHOLDER_NEWCOMMAND_97>\n<PLACEHOLDER_NEWCOMMAND_98>\n<PLACEHOLDER_NEWCOMMAND_99>\n<PLACEHOLDER_NEWCOMMAND_100>\n<PLACEHOLDER_NEWCOMMAND_101>\n<PLACEHOLDER_NEWCOMMAND_102>\n\n<PLACEHOLDER_NEWCOMMAND_103>\n<PLACEHOLDER_NEWCOMMAND_104>\n<PLACEHOLDER_NEWCOMMAND_105>\n<PLACEHOLDER_NEWCOMMAND_106>\n<PLACEHOLDER_NEWCOMMAND_107>\n<PLACEHOLDER_NEWCOMMAND_108>\n<PLACEHOLDER_NEWCOMMAND_109>\n<PLACEHOLDER_NEWCOMMAND_110>\n<PLACEHOLDER_NEWCOMMAND_111>\n<PLACEHOLDER_NEWCOMMAND_112>\n\n<PLACEHOLDER_NEWCOMMAND_113>\n\\let\\Re\\relax\n\n\\DeclareMathOperator{\\Re}{\\Rr e}\n\\DeclareMathOperator{\\Imag}{\\Ii m}\n\\DeclareMathOperator{\\Ker}{Ker}\n\\DeclareMathOperator{\\Hom}{Hom}\n\\DeclareMathOperator{\\End}{End}\n\\DeclareMathOperator{\\tr}{tr}\n\\DeclareMathOperator{\\Tr}{Tr}\n\\DeclareMathOperator{\\Supp}{Supp}\n\\DeclareMathOperator{\\Sign}{Sign}\n\\let\\Im\\relax\n\\DeclareMathOperator{\\Im}{Im}\n\\DeclareMathOperator{\\Corr}{Corr}\n\\DeclareMathOperator{\\sign}{sign}\n\\DeclareMathOperator{\\supp}{supp}\n\n\\DeclareMathOperator{\\cas}{cas}\n\\DeclareMathOperator{\\sinc}{sinc}\n\\DeclareMathOperator{\\cotan}{cotan}\n\\DeclareMathOperator{\\Card}{Card}\n\\DeclareMathOperator{\\GCD}{GCD}\n\\DeclareMathOperator{\\grad}{grad}\n\\DeclareMathOperator{\\Diag}{Diag}\n\\DeclareMathOperator{\\rank}{rank}\n\\DeclareMathOperator{\\conv}{conv}\n\\DeclareMathOperator{\\interop}{int}\n\n<PLACEHOLDER_NEWCOMMAND_114>\n\n<PLACEHOLDER_NEWCOMMAND_115>\n<PLACEHOLDER_NEWCOMMAND_116>\n<PLACEHOLDER_NEWCOMMAND_117>\n<PLACEHOLDER_NEWCOMMAND_118>\n<PLACEHOLDER_NEWCOMMAND_119>\n<PLACEHOLDER_NEWCOMMAND_120>\n<PLACEHOLDER_NEWCOMMAND_121>\n<PLACEHOLDER_NEWCOMMAND_122>\n<PLACEHOLDER_NEWCOMMAND_123>\n<PLACEHOLDER_NEWCOMMAND_124>\n<PLACEHOLDER_NEWCOMMAND_125>\n<PLACEHOLDER_NEWCOMMAND_126>\n<PLACEHOLDER_NEWCOMMAND_127>\n<PLACEHOLDER_NEWCOMMAND_128>\n<PLACEHOLDER_NEWCOMMAND_129>\n<PLACEHOLDER_NEWCOMMAND_130>\n<PLACEHOLDER_NEWCOMMAND_131>\n<PLACEHOLDER_NEWCOMMAND_132>\n<PLACEHOLDER_NEWCOMMAND_133>\n\n<PLACEHOLDER_NEWCOMMAND_134>\n<PLACEHOLDER_NEWCOMMAND_135>\n<PLACEHOLDER_NEWCOMMAND_136>\n<PLACEHOLDER_NEWCOMMAND_137>\n<PLACEHOLDER_NEWCOMMAND_138>\n<PLACEHOLDER_NEWCOMMAND_139>\n<PLACEHOLDER_NEWCOMMAND_140>\n<PLACEHOLDER_NEWCOMMAND_141>\n<PLACEHOLDER_NEWCOMMAND_142>\n\n<PLACEHOLDER_NEWCOMMAND_143>\n<PLACEHOLDER_NEWCOMMAND_144>\n<PLACEHOLDER_NEWCOMMAND_145>\n<PLACEHOLDER_NEWCOMMAND_146>\n<PLACEHOLDER_NEWCOMMAND_147>\n<PLACEHOLDER_NEWCOMMAND_148>\n<PLACEHOLDER_NEWCOMMAND_149>\n<PLACEHOLDER_NEWCOMMAND_150>\n<PLACEHOLDER_NEWCOMMAND_151>\n<PLACEHOLDER_NEWCOMMAND_152>\n<PLACEHOLDER_NEWCOMMAND_153>\n\\DeclareMathOperator{\\diverg}{div}\n\\DeclareMathOperator{\\Prox}{Prox}\n<PLACEHOLDER_NEWCOMMAND_154>\n<PLACEHOLDER_NEWCOMMAND_155>\n<PLACEHOLDER_NEWCOMMAND_156>\n<PLACEHOLDER_NEWCOMMAND_157>\n<PLACEHOLDER_NEWCOMMAND_158>\n<PLACEHOLDER_NEWCOMMAND_159>\n<PLACEHOLDER_NEWCOMMAND_160>\n\n\\newcommand{\\func}[4]{ {\\left\\{  <PLACEHOLDER_ENV_2>  \\right.} }\n<PLACEHOLDER_NEWCOMMAND_161>\n<PLACEHOLDER_NEWCOMMAND_162>\n\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\n<PLACEHOLDER_NEWCOMMAND_163>\n<PLACEHOLDER_NEWCOMMAND_164>\n\\newcommand{\\choice}[1]{\n\t\\left\\{\n\t\t<PLACEHOLDER_ENV_3>\n\t\\right. }\n<PLACEHOLDER_NEWCOMMAND_165>\n<PLACEHOLDER_NEWCOMMAND_166>\n\n<PLACEHOLDER_NEWCOMMAND_167>\n<PLACEHOLDER_NEWCOMMAND_168>\n<PLACEHOLDER_NEWCOMMAND_169>\n<PLACEHOLDER_NEWCOMMAND_170>\n<PLACEHOLDER_NEWCOMMAND_171>\n<PLACEHOLDER_NEWCOMMAND_172>\n<PLACEHOLDER_NEWCOMMAND_173>\n<PLACEHOLDER_NEWCOMMAND_174>\n<PLACEHOLDER_NEWCOMMAND_175>\n<PLACEHOLDER_NEWCOMMAND_176>\n<PLACEHOLDER_NEWCOMMAND_177>\n<PLACEHOLDER_NEWCOMMAND_178>\n<PLACEHOLDER_NEWCOMMAND_179>\n<PLACEHOLDER_NEWCOMMAND_180>\n<PLACEHOLDER_NEWCOMMAND_181>\n<PLACEHOLDER_NEWCOMMAND_182>\n<PLACEHOLDER_NEWCOMMAND_183>\n<PLACEHOLDER_NEWCOMMAND_184>\n<PLACEHOLDER_NEWCOMMAND_185>\n<PLACEHOLDER_NEWCOMMAND_186>\n\n\\newlength{\\restsubwidth}\n\\newlength{\\restsubheight}\n\\newlength{\\restsubmoreheight}\n\\setlength{\\restsubmoreheight}{4pt}\n<PLACEHOLDER_NEWCOMMAND_187><PLACEHOLDER_commands.tex_end>\n\n<PLACEHOLDER_NEWCOMMAND_188>\n<PLACEHOLDER_NEWCOMMAND_189>\n<PLACEHOLDER_NEWCOMMAND_190>\n<PLACEHOLDER_NEWCOMMAND_191>\n<PLACEHOLDER_NEWCOMMAND_192>\n\n<PLACEHOLDER_ENV_4>"
    },
    {
        "section": "0",
        "content": "\\begin{document}\n\n\\maketitle\n<PLACEHOLDER_ENV_5>\n\n\\newpage\n\n<PLACEHOLDER_ENV_6>\n\n\\newpage",
        "trans_content": "\\begin{document}\n\n\\maketitle\n<PLACEHOLDER_ENV_5>\n\n\\newpage\n\n<PLACEHOLDER_ENV_6>\n\n\\newpage"
    },
    {
        "section": "1",
        "content": "\\section{Introduction}\nIn recent years, Large Language Models~(LLMs) have been undergoing rapid iteration and evolution~\\citep{gpt4o,claude35sonnet,gemini1_5}, progressively diminishing the gap towards Artificial General Intelligence~(AGI).\n\nRecently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1~\\citep{o1} series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community.\nSeveral prior works have explored various approaches, including process-based reward models \\citep{uesato2022solving, lightman2023let,mathshepherd}, reinforcement learning \\citep{kumar2024training}, and search algorithms such as Monte Carlo Tree Search and Beam Search \\citep{feng2024alphazeroliketreesearchguidelarge,xin2024deepseekproverv15harnessingproofassistant,AlphaGeometryTrinh2024}.\nHowever, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models.\n\nIn this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL).\nOur goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process.\nSpecifically, we use DeepSeek-V3-Base as the base model and employ GRPO~\\citep{deepseekmath} as the RL framework to improve model performance in reasoning.\nDuring training, \\dsro{} naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, \\dsro{} exhibits super performance on reasoning benchmarks.\nFor instance, the pass@1 score on AIME 2024 increases from 15.6\\% to 71.0\\%, and with majority voting, the score further improves to 86.7\\%, matching the performance of OpenAI-o1-0912.\n\nHowever, \\dsro{} encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce \\dsri{}, which incorporates a small amount of cold-start data and a multi-stage training pipeline.\nSpecifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like \\dsro{}. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as \\dsri{}, which achieves performance on par with OpenAI-o1-1217.\n\nWe further explore distillation from \\dsri{} to smaller dense models. Using Qwen2.5-32B~\\citep{qwen2_5} as the base model, direct distillation from \\dsri{} outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities.  We open-source the distilled Qwen and Llama~\\citep{llama3} series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview~\\citep{QwQ} by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models.",
        "trans_content": "\\section{Introduction}\nIn recent years, Large Language Models~(LLMs) have been undergoing rapid iteration and evolution~\\citep{gpt4o,claude35sonnet,gemini1_5}, progressively diminishing the gap towards Artificial General Intelligence~(AGI).\n\nRecently, post-training has emerged as an important component of the full training pipeline. It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt to user preferences, all while requiring relatively minimal computational resources against pre-training. In the context of reasoning capabilities, OpenAI's o1~\\citep{o1} series models were the first to introduce inference-time scaling by increasing the length of the Chain-of-Thought reasoning process. This approach has achieved significant improvements in various reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge of effective test-time scaling remains an open question for the research community.\nSeveral prior works have explored various approaches, including process-based reward models \\citep{uesato2022solving, lightman2023let,mathshepherd}, reinforcement learning \\citep{kumar2024training}, and search algorithms such as Monte Carlo Tree Search and Beam Search \\citep{feng2024alphazeroliketreesearchguidelarge,xin2024deepseekproverv15harnessingproofassistant,AlphaGeometryTrinh2024}.\nHowever, none of these methods has achieved general reasoning performance comparable to OpenAI's o1 series models.\n\nIn this paper, we take the first step toward improving language model reasoning capabilities using pure reinforcement learning (RL).\nOur goal is to explore the potential of LLMs to develop reasoning capabilities without any supervised data, focusing on their self-evolution through a pure RL process.\nSpecifically, we use DeepSeek-V3-Base as the base model and employ GRPO~\\citep{deepseekmath} as the RL framework to improve model performance in reasoning.\nDuring training, \\dsro{} naturally emerged with numerous powerful and interesting reasoning behaviors. After thousands of RL steps, \\dsro{} exhibits super performance on reasoning benchmarks.\nFor instance, the pass@1 score on AIME 2024 increases from 15.6\\% to 71.0\\%, and with majority voting, the score further improves to 86.7\\%, matching the performance of OpenAI-o1-0912.\n\nHowever, \\dsro{} encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce \\dsri{}, which incorporates a small amount of cold-start data and a multi-stage training pipeline.\nSpecifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like \\dsro{}. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model. After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to as \\dsri{}, which achieves performance on par with OpenAI-o1-1217.\n\nWe further explore distillation from \\dsri{} to smaller dense models. Using Qwen2.5-32B~\\citep{qwen2_5} as the base model, direct distillation from \\dsri{} outperforms applying RL on it. This demonstrates that the reasoning patterns discovered by larger base models are crucial for improving reasoning capabilities.  We open-source the distilled Qwen and Llama~\\citep{llama3} series. Notably, our distilled 14B model outperforms state-of-the-art open-source QwQ-32B-Preview~\\citep{QwQ} by a large margin, and the distilled 32B and 70B models set a new record on the reasoning benchmarks among dense models."
    },
    {
        "section": "1_1",
        "content": "\\subsection{Contributions}\n\n\\paragraph{Post-Training: Large-Scale Reinforcement Learning on the Base Model}\n<PLACEHOLDER_ENV_7>\n\n\\paragraph{Distillation: Smaller Models Can Be Powerful Too}\n<PLACEHOLDER_ENV_8>",
        "trans_content": "\\subsection{Contributions}\n\n\\paragraph{Post-Training: Large-Scale Reinforcement Learning on the Base Model}\n<PLACEHOLDER_ENV_7>\n\n\\paragraph{Distillation: Smaller Models Can Be Powerful Too}\n<PLACEHOLDER_ENV_8>"
    },
    {
        "section": "1_2",
        "content": "\\subsection{Summary of Evaluation Results}\n<PLACEHOLDER_ENV_9>",
        "trans_content": "\\subsection{Summary of Evaluation Results}\n<PLACEHOLDER_ENV_9>"
    },
    {
        "section": "2+2_1",
        "content": "\\section{Approach}\n\n\\subsection{Overview}\nPrevious work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) \\dsro{}, which applies RL directly to the base model without any SFT data, and (2) \\dsri{}, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from \\dsri{} to small dense models.",
        "trans_content": "\\section{Approach}\n\n\\subsection{Overview}\nPrevious work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) \\dsro{}, which applies RL directly to the base model without any SFT data, and (2) \\dsri{}, which applies RL starting from a checkpoint fine-tuned with thousands of long Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from \\dsri{} to small dense models."
    },
    {
        "section": "2_2",
        "content": "\\subsection{ \\dsro{}: Reinforcement Learning on the Base Model}\n\\label{sec:ds-zero}\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works \\citep{mathshepherd,deepseekmath}.\nHowever, these works heavily depended on supervised data, which are time-intensive to gather.\nIn this section, we explore the potential of LLMs to develop reasoning capabilities \\textbf{without any supervised data}, focusing on their self-evolution through a pure reinforcement learning process.\nWe start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights.",
        "trans_content": "\\subsection{ \\dsro{}: Reinforcement Learning on the Base Model}\n\\label{sec:ds-zero}\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as evidenced by our previous works \\citep{mathshepherd,deepseekmath}.\nHowever, these works heavily depended on supervised data, which are time-intensive to gather.\nIn this section, we explore the potential of LLMs to develop reasoning capabilities \\textbf{without any supervised data}, focusing on their self-evolution through a pure reinforcement learning process.\nWe start with a brief overview of our RL algorithm, followed by the presentation of some exciting results, and hope this provides the community with valuable insights."
    },
    {
        "section": "2_2_1",
        "content": "\\subsubsection{Reinforcement Learning Algorithm}\n\\paragraph{Group Relative Policy Optimization} In order to save the training costs of RL, we adopt Group Relative Policy Optimization~(GRPO) \\citep{deepseekmath}, which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead.\nSpecifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_G\\}$ from the old policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model $\\pi_{\\theta}$ by maximizing the following objective:\n<PLACEHOLDER_ENV_10>\n<PLACEHOLDER_ENV_11>\nwhere $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_i$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group:\n<PLACEHOLDER_ENV_12>\n\n<PLACEHOLDER_ENV_13>",
        "trans_content": "\\subsubsection{Reinforcement Learning Algorithm}\n\\paragraph{Group Relative Policy Optimization} In order to save the training costs of RL, we adopt Group Relative Policy Optimization~(GRPO) \\citep{deepseekmath}, which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead.\nSpecifically, for each question $q$, GRPO samples a group of outputs $\\{o_1, o_2, \\cdots, o_G\\}$ from the old policy $\\pi_{\\theta_{old}}$ and then optimizes the policy model $\\pi_{\\theta}$ by maximizing the following objective:\n<PLACEHOLDER_ENV_10>\n<PLACEHOLDER_ENV_11>\nwhere $\\epsilon$ and $\\beta$ are hyper-parameters, and $A_i$ is the advantage, computed using a group of rewards $\\{r_1, r_2, \\ldots, r_G\\}$ corresponding to the outputs within each group:\n<PLACEHOLDER_ENV_12>\n\n<PLACEHOLDER_ENV_13>"
    },
    {
        "section": "2_2_2",
        "content": "\\subsubsection{Reward Modeling} The reward is the source of the training signal, which decides the optimization direction of RL.\nTo train \\dsro{}, we adopt a rule-based reward system that mainly consists of two types of rewards:\n<PLACEHOLDER_ENV_14>\nWe do not apply the outcome or process neural reward model in developing  \\dsro{}, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline.",
        "trans_content": "\\subsubsection{Reward Modeling} The reward is the source of the training signal, which decides the optimization direction of RL.\nTo train \\dsro{}, we adopt a rule-based reward system that mainly consists of two types of rewards:\n<PLACEHOLDER_ENV_14>\nWe do not apply the outcome or process neural reward model in developing  \\dsro{}, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process, and retraining the reward model needs additional training resources and it complicates the whole training pipeline."
    },
    {
        "section": "2_2_3",
        "content": "\\subsubsection{Training Template}\n\nTo train \\dsro{}, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table \\ref{tab:r0_template}, this template requires \\dsro{} to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biases—such as mandating reflective reasoning or promoting particular problem-solving strategies—to ensure that we can accurately observe the model's natural progression during the RL process.",
        "trans_content": "\\subsubsection{Training Template}\n\nTo train \\dsro{}, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table \\ref{tab:r0_template}, this template requires \\dsro{} to first produce a reasoning process, followed by the final answer. We intentionally limit our constraints to this structural format, avoiding any content-specific biases—such as mandating reflective reasoning or promoting particular problem-solving strategies—to ensure that we can accurately observe the model's natural progression during the RL process."
    },
    {
        "section": "2_2_4",
        "content": "\\subsubsection{Performance, Self-evolution Process and Aha Moment of \\dsro{}}\n\n\\paragraph{Performance of \\dsro{}}\n\n<PLACEHOLDER_ENV_15>\n\n<PLACEHOLDER_ENV_16>\n\nFigure \\ref{fig:zero-training-performance} depicts the performance trajectory of \\dsro{} on the AIME 2024 benchmark throughout the RL training process. As illustrated, \\dsro{} demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6\\% to an impressive 71.0\\%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the model's performance over time.\n\nTable \\ref{tab:r1-zero} provides a comparative analysis between \\dsro{} and OpenAI's o1-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers \\dsro{} to attain robust reasoning capabilities without the need for any supervised fine-tuning data.\nThis is a noteworthy achievement, as it underscores the model's ability to learn and generalize effectively through RL alone. Additionally, the performance of \\dsro{} can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, \\dsro{}'s performance escalates from 71.0\\% to 86.7\\%, thereby exceeding the performance of OpenAI-o1-0912.\nThe ability of \\dsro{} to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.\n\n<PLACEHOLDER_ENV_17>\n\n\\paragraph{Self-evolution Process of \\dsro{}}\nThe self-evolution process of \\dsro{} is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the model's progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.\n\nAs depicted in Figure \\ref{fig:zero-training-length}, the thinking time of \\dsro{} shows consistent improvement throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model. \\dsro{} naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.\n\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflection—where the model revisits and reevaluates its previous steps—and the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model's interaction with the reinforcement learning environment. This spontaneous development significantly enhances \\dsro{}'s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.\n\n\\paragraph{Aha Moment of \\dsro{}}\nA particularly intriguing phenomenon observed during the training of \\dsro{} is the occurrence of an ``aha moment''. This moment, as illustrated in Table \\ref{tab:aha_moment}, occurs in an intermediate version of the model. During this phase, \\dsro{} learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model's growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\n\nThis moment is not only an ``aha moment'' for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies.\nThe ``aha moment'' serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.\n\n\\paragraph{Drawback of \\dsro{}}\nAlthough \\dsro{} exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, \\dsro{} struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore \\dsri{}, a method that utilizes RL with human-friendly cold-start data.\n\n<PLACEHOLDER_ENV_18>",
        "trans_content": "\\subsubsection{Performance, Self-evolution Process and Aha Moment of \\dsro{}}\n\n\\paragraph{Performance of \\dsro{}}\n\n<PLACEHOLDER_ENV_15>\n\n<PLACEHOLDER_ENV_16>\n\nFigure \\ref{fig:zero-training-performance} depicts the performance trajectory of \\dsro{} on the AIME 2024 benchmark throughout the RL training process. As illustrated, \\dsro{} demonstrates a steady and consistent enhancement in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumping from an initial 15.6\\% to an impressive 71.0\\%, reaching performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL algorithm in optimizing the model's performance over time.\n\nTable \\ref{tab:r1-zero} provides a comparative analysis between \\dsro{} and OpenAI's o1-0912 models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers \\dsro{} to attain robust reasoning capabilities without the need for any supervised fine-tuning data.\nThis is a noteworthy achievement, as it underscores the model's ability to learn and generalize effectively through RL alone. Additionally, the performance of \\dsro{} can be further augmented through the application of majority voting. For example, when majority voting is employed on the AIME benchmark, \\dsro{}'s performance escalates from 71.0\\% to 86.7\\%, thereby exceeding the performance of OpenAI-o1-0912.\nThe ability of \\dsro{} to achieve such competitive performance, both with and without majority voting, highlights its strong foundational capabilities and its potential for further advancements in reasoning tasks.\n\n<PLACEHOLDER_ENV_17>\n\n\\paragraph{Self-evolution Process of \\dsro{}}\nThe self-evolution process of \\dsro{} is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities autonomously. By initiating RL directly from the base model, we can closely monitor the model's progression without the influence of the supervised fine-tuning stage. This approach provides a clear view of how the model evolves over time, particularly in terms of its ability to handle complex reasoning tasks.\n\nAs depicted in Figure \\ref{fig:zero-training-length}, the thinking time of \\dsro{} shows consistent improvement throughout the training process. This improvement is not the result of external adjustments but rather an intrinsic development within the model. \\dsro{} naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens, allowing the model to explore and refine its thought processes in greater depth.\n\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated behaviors as the test-time computation increases. Behaviors such as reflection—where the model revisits and reevaluates its previous steps—and the exploration of alternative approaches to problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead emerge as a result of the model's interaction with the reinforcement learning environment. This spontaneous development significantly enhances \\dsro{}'s reasoning capabilities, enabling it to tackle more challenging tasks with greater efficiency and accuracy.\n\n\\paragraph{Aha Moment of \\dsro{}}\nA particularly intriguing phenomenon observed during the training of \\dsro{} is the occurrence of an ``aha moment''. This moment, as illustrated in Table \\ref{tab:aha_moment}, occurs in an intermediate version of the model. During this phase, \\dsro{} learns to allocate more thinking time to a problem by reevaluating its initial approach. This behavior is not only a testament to the model's growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticated outcomes.\n\nThis moment is not only an ``aha moment'' for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies.\nThe ``aha moment'' serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.\n\n\\paragraph{Drawback of \\dsro{}}\nAlthough \\dsro{} exhibits strong reasoning capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces several issues. For instance, \\dsro{} struggles with challenges like poor readability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore \\dsri{}, a method that utilizes RL with human-friendly cold-start data.\n\n<PLACEHOLDER_ENV_18>"
    },
    {
        "section": "2_3",
        "content": "\\subsection{\\dsri{}: Reinforcement Learning with Cold Start}\nInspired by the promising results of \\dsro{}, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities?\nTo address these questions, we design a pipeline to train \\dsri{}. The pipeline consists of four stages, outlined as follows.",
        "trans_content": "\\subsection{\\dsri{}: Reinforcement Learning with Cold Start}\nInspired by the promising results of \\dsro{}, two natural questions arise: 1) Can reasoning performance be further improved or convergence accelerated by incorporating a small amount of high-quality data as a cold start? 2) How can we train a user-friendly model that not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong general capabilities?\nTo address these questions, we design a pipeline to train \\dsri{}. The pipeline consists of four stages, outlined as follows."
    },
    {
        "section": "2_3_1",
        "content": "\\subsubsection{Cold Start}\nUnlike \\dsro{}, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor.\nTo collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering \\dsro{} outputs in a readable format, and refining the results through post-processing by human annotators.\n\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL.\nCompared to \\dsro{, the advantages of cold start data include:\n<PLACEHOLDER_ENV_19>",
        "trans_content": "\\subsubsection{Cold Start}\nUnlike \\dsro{}, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor.\nTo collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering \\dsro{} outputs in a readable format, and refining the results through post-processing by human annotators.\n\nIn this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL.\nCompared to \\dsro{, the advantages of cold start data include:\n<PLACEHOLDER_ENV_19>"
    },
    {
        "section": "2_3_2",
        "content": "\\subsubsection{Reasoning-oriented Reinforcement Learning}\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in \\dsro. This phase focuses on enhancing the model's reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the model's performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks.",
        "trans_content": "\\subsubsection{Reasoning-oriented Reinforcement Learning}\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in \\dsro. This phase focuses on enhancing the model's reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions. During the training process, we observe that CoT often exhibits language mixing, particularly when RL prompts involve multiple languages. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT. Although ablation experiments show that such alignment results in a slight degradation in the model's performance, this reward aligns with human preferences, making it more readable. Finally, we combine the accuracy of reasoning tasks and the reward for language consistency by directly summing them to form the final reward. We then apply RL training on the fine-tuned model until it achieves convergence on reasoning tasks."
    },
    {
        "section": "2_3_3",
        "content": "\\subsubsection{Rejection Sampling and Supervised Fine-Tuning}\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model's capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below.\n\\label{sec:method:r1:sft}\n\n\\paragraph{Reasoning data}\nWe curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training.\nIn the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\nAdditionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks.\nFor each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.\n\n\\paragraph{Non-Reasoning data}\nFor non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ``hello'' we do not provide a CoT in response.\nIn the end, we collected a total of approximately 200k training samples that are unrelated to reasoning.\n\nWe fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples.",
        "trans_content": "\\subsubsection{Rejection Sampling and Supervised Fine-Tuning}\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the model's capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we generate the data and fine-tune the model as described below.\n\\label{sec:method:r1:sft}\n\n\\paragraph{Reasoning data}\nWe curate reasoning prompts and generate reasoning trajectories by performing rejection sampling from the checkpoint from the above RL training.\nIn the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\nAdditionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks.\nFor each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.\n\n\\paragraph{Non-Reasoning data}\nFor non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as ``hello'' we do not provide a CoT in response.\nIn the end, we collected a total of approximately 200k training samples that are unrelated to reasoning.\n\nWe fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples."
    },
    {
        "section": "2_3_4",
        "content": "\\subsubsection{Reinforcement Learning for all Scenarios}\n\nTo further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model's helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions.\nFor reasoning data, we adhere to the methodology outlined in \\dsro, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains.\nFor general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.",
        "trans_content": "\\subsubsection{Reinforcement Learning for all Scenarios}\n\nTo further align the model with human preferences, we implement a secondary reinforcement learning stage aimed at improving the model's helpfulness and harmlessness while simultaneously refining its reasoning capabilities. Specifically, we train the model using a combination of reward signals and diverse prompt distributions.\nFor reasoning data, we adhere to the methodology outlined in \\dsro, which utilizes rule-based rewards to guide the learning process in math, code, and logical reasoning domains.\nFor general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process. For harmlessness, we evaluate the entire response of the model, including both the reasoning process and the summary, to identify and mitigate any potential risks, biases, or harmful content that may arise during the generation process.\nUltimately, the integration of reward signals and diverse data distributions enables us to train a model that excels in reasoning while prioritizing helpfulness and harmlessness."
    },
    {
        "section": "2_4",
        "content": "\\subsection{Distillation: Empower Small Models with Reasoning Capability }\n\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen \\citep{qwen2_5} and Llama \\citep{llama3_1_405b} using the 800k samples curated with DeepSeek-R1, as detailed in \\S \\ref{sec:method:r1:sft}.\nOur findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models.\nThe base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1.\n\nFor distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community.",
        "trans_content": "\\subsection{Distillation: Empower Small Models with Reasoning Capability }\n\nTo equip more efficient smaller models with reasoning capabilities like DeepSeek-R1, we directly fine-tuned open-source models like Qwen \\citep{qwen2_5} and Llama \\citep{llama3_1_405b} using the 800k samples curated with DeepSeek-R1, as detailed in \\S \\ref{sec:method:r1:sft}.\nOur findings indicate that this straightforward distillation method significantly enhances the reasoning abilities of smaller models.\nThe base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its reasoning capability is slightly better than that of Llama-3.1.\n\nFor distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stage to the broader research community."
    },
    {
        "section": "3",
        "content": "\\section{Experiment}\n\n\\paragraph{Benchmarks} We evaluate models on MMLU \\citep{mmlu}, MMLU-Redux \\citep{mmlu_redux}, MMLU-Pro \\citep{mmlu_pro}, C-Eval \\citep{ceval}, and CMMLU \\citep{cmmlu}, IFEval~\\citep{IFeval}, FRAMES~\\citep{frames}, GPQA Diamond ~\\citep{gpqa}, SimpleQA~\\citep{simpleqa}, C-SimpleQA~\\citep{csimpleqa}, SWE-Bench Verified~\\citep{swe_verified}, Aider~\\footnote{\\url{https://aider.chat}}, LiveCodeBench~\\citep{livecodebench} (2024-08 -- 2025-01), Codeforces~\\footnote{\\url{https://codeforces.com}}, Chinese National High School Mathematics Olympiad (CNMO 2024)\\footnote{\\url{https://www.cms.org.cn/Home/comp/comp/cid/12.html}}, and American Invitational Mathematics Examination 2024 (AIME 2024)~\\citep{AIME2024}.\nIn addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges.\nSpecifically, we adhere to the original configurations of AlpacaEval 2.0~\\citep{alpaca2.0} and Arena-Hard~\\citep{li2024crowdsourced}, which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias.\nFor distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.\n\n\\paragraph{Evaluation Prompts} Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format~\\citep{Lin_ZeroEval_A_Unified_2024} in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of  \\dsri{}.\nOther datasets follow their original evaluation protocols with default prompts provided by their creators.\nFor code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C\\#, JavaScript, TypeScript, PHP, and Bash).\nModel performance on LiveCodeBench is evaluated using  CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework~\\citep{agentless}. AIDER-related benchmarks are measured using a \"diff\" format.\n\\dsri{} outputs are capped at a maximum of 32,768 tokens for each benchmark.\n\n\\paragraph{Baselines} We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports.\nFor distilled models, we also compare the open-source model QwQ-32B-Preview \\citep{QwQ}.\n\n\\paragraph{Evaluation Setup}\nWe set the maximum generation length to 32,768 tokens for the models.\nWe found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints.\nTherefore, we default to pass@$k$ evaluation \\citep{codex} and report pass@1 using a non-zero temperature.\nSpecifically, we use a sampling temperature of $0.6$ and a top-$p$ value of $0.95$ to generate $k$ responses (typically between $4$ and $64$, depending on the test set size) for each question. Pass@1 is then calculated as\n\\[\n\\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i,\n\\]\nwhere $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates.\nFor AIME 2024, we also report consensus (majority vote) results \\citep{wang2022self} using $64$ samples, denoted as $\\text{cons}@64$.",
        "trans_content": "\\section{Experiment}\n\n\\paragraph{Benchmarks} We evaluate models on MMLU \\citep{mmlu}, MMLU-Redux \\citep{mmlu_redux}, MMLU-Pro \\citep{mmlu_pro}, C-Eval \\citep{ceval}, and CMMLU \\citep{cmmlu}, IFEval~\\citep{IFeval}, FRAMES~\\citep{frames}, GPQA Diamond ~\\citep{gpqa}, SimpleQA~\\citep{simpleqa}, C-SimpleQA~\\citep{csimpleqa}, SWE-Bench Verified~\\citep{swe_verified}, Aider~\\footnote{\\url{https://aider.chat}}, LiveCodeBench~\\citep{livecodebench} (2024-08 -- 2025-01), Codeforces~\\footnote{\\url{https://codeforces.com}}, Chinese National High School Mathematics Olympiad (CNMO 2024)\\footnote{\\url{https://www.cms.org.cn/Home/comp/comp/cid/12.html}}, and American Invitational Mathematics Examination 2024 (AIME 2024)~\\citep{AIME2024}.\nIn addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges.\nSpecifically, we adhere to the original configurations of AlpacaEval 2.0~\\citep{alpaca2.0} and Arena-Hard~\\citep{li2024crowdsourced}, which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we only feed the final summary to evaluation to avoid the length bias.\nFor distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench.\n\n\\paragraph{Evaluation Prompts} Following the setup in DeepSeek-V3, standard benchmarks such as MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format~\\citep{Lin_ZeroEval_A_Unified_2024} in a zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of  \\dsri{}.\nOther datasets follow their original evaluation protocols with default prompts provided by their creators.\nFor code and math benchmarks, the HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++, C\\#, JavaScript, TypeScript, PHP, and Bash).\nModel performance on LiveCodeBench is evaluated using  CoT format, with data collected between August 2024 and January 2025. The Codeforces dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework~\\citep{agentless}. AIDER-related benchmarks are measured using a \"diff\" format.\n\\dsri{} outputs are capped at a maximum of 32,768 tokens for each benchmark.\n\n\\paragraph{Baselines} We conduct comprehensive evaluations against several strong baselines, including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217. Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its performance based on official reports.\nFor distilled models, we also compare the open-source model QwQ-32B-Preview \\citep{QwQ}.\n\n\\paragraph{Evaluation Setup}\nWe set the maximum generation length to 32,768 tokens for the models.\nWe found that using greedy decoding to evaluate long-output reasoning models results in higher repetition rates and significant variability across different checkpoints.\nTherefore, we default to pass@$k$ evaluation \\citep{codex} and report pass@1 using a non-zero temperature.\nSpecifically, we use a sampling temperature of $0.6$ and a top-$p$ value of $0.95$ to generate $k$ responses (typically between $4$ and $64$, depending on the test set size) for each question. Pass@1 is then calculated as\n\\[\n\\text{pass@1} = \\frac{1}{k} \\sum_{i=1}^{k} p_i,\n\\]\nwhere $p_i$ denotes the correctness of the $i$-th response. This method provides more reliable performance estimates.\nFor AIME 2024, we also report consensus (majority vote) results \\citep{wang2022self} using $64$ samples, denoted as $\\text{cons}@64$."
    },
    {
        "section": "3_1",
        "content": "\\subsection{\\dsri{} Evaluation}\n\n<PLACEHOLDER_tables/chateval_begin><PLACEHOLDER_ENV_20><PLACEHOLDER_tables/chateval_end>\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, \\dsri{} demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, \\dsri{} excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, \\dsri{} outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, \\dsri{} performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, \\dsri{} could achieve an accuracy of over 70\\%.\n\n\\dsri{} also delivers impressive results on IF-Eval, a benchmark designed to assess a model's ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating \\dsri{}’s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains.\nMoreover, the summary lengths generated by \\dsri{} are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that \\dsri{} avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks.\n\nOn math tasks, \\dsri{} demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms \\dsri{} on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of \\dsri{} will improve in the next version, as the amount of related RL training data currently remains very limited.",
        "trans_content": "\\subsection{\\dsri{} Evaluation}\n\n<PLACEHOLDER_tables/chateval_begin><PLACEHOLDER_ENV_20><PLACEHOLDER_tables/chateval_end>\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, \\dsri{} demonstrates superior performance compared to DeepSeek-V3. This improvement is primarily attributed to enhanced accuracy in STEM-related questions, where significant gains are achieved through large-scale reinforcement learning. Additionally, \\dsri{} excels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysis tasks. On the factual benchmark SimpleQA, \\dsri{} outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, \\dsri{} performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answering certain queries after safety RL. Without safety RL, \\dsri{} could achieve an accuracy of over 70\\%.\n\n\\dsri{} also delivers impressive results on IF-Eval, a benchmark designed to assess a model's ability to follow format instructions. These improvements can be linked to the inclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard, indicating \\dsri{}’s strengths in writing tasks and open-domain question answering. Its significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale RL, which not only boosts reasoning capabilities but also improves performance across diverse domains.\nMoreover, the summary lengths generated by \\dsri{} are concise, with an average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that \\dsri{} avoids introducing length bias during GPT-based evaluations, further solidifying its robustness across multiple tasks.\n\nOn math tasks, \\dsri{} demonstrates performance on par with OpenAI-o1-1217, surpassing other models by a large margin. A similar trend is observed on coding algorithm tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms \\dsri{} on Aider but achieves comparable performance on SWE Verified. We believe the engineering performance of \\dsri{} will improve in the next version, as the amount of related RL training data currently remains very limited."
    },
    {
        "section": "3_2",
        "content": "\\subsection{Distilled Model Evaluation}\n\\label{sec:distilled_model_evaluation}\n<PLACEHOLDER_tables/distill_eval_begin><PLACEHOLDER_ENV_21>\n<PLACEHOLDER_tables/distill_eval_end>\n\nAs shown in Table \\ref{tab:distill}, simply distilling DeepSeek-R1's outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board.\nDeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks.\nThese results demonstrate the strong potential of distillation.\nAdditionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.",
        "trans_content": "\\subsection{Distilled Model Evaluation}\n\\label{sec:distilled_model_evaluation}\n<PLACEHOLDER_tables/distill_eval_begin><PLACEHOLDER_ENV_21>\n<PLACEHOLDER_tables/distill_eval_end>\n\nAs shown in Table \\ref{tab:distill}, simply distilling DeepSeek-R1's outputs enables the efficient DeepSeek-R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-reasoning models like GPT-4o-0513 across the board.\nDeepSeek-R1-14B surpasses QwQ-32B-Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly exceed o1-mini on most benchmarks.\nThese results demonstrate the strong potential of distillation.\nAdditionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here."
    },
    {
        "section": "4+4_1",
        "content": "\\section{Discussion}\n\n\\subsection{Distillation v.s. Reinforcement Learning}\n<PLACEHOLDER_tables/distill_vs_rl_begin><PLACEHOLDER_ENV_22><PLACEHOLDER_tables/distill_vs_rl_end>\n\nIn Section \\ref{sec:distilled_model_evaluation}, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?\n\nTo answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table \\ref{tab:distill_vs_rl}, demonstrate that the 32B base model, after large-scale RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1,  performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks.\n\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning.",
        "trans_content": "\\section{Discussion}\n\n\\subsection{Distillation v.s. Reinforcement Learning}\n<PLACEHOLDER_tables/distill_vs_rl_begin><PLACEHOLDER_ENV_22><PLACEHOLDER_tables/distill_vs_rl_end>\n\nIn Section \\ref{sec:distilled_model_evaluation}, we can see that by distilling DeepSeek-R1, the small model can achieve impressive results. However, there is still one question left: can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?\n\nTo answer this question, we conduct large-scale RL training on Qwen-32B-Base using math, code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The experimental results, shown in Table \\ref{tab:distill_vs_rl}, demonstrate that the 32B base model, after large-scale RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-Distill-Qwen-32B, which is distilled from DeepSeek-R1,  performs significantly better than DeepSeek-R1-Zero-Qwen-32B across all benchmarks.\n\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller ones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in this paper require enormous computational power and may not even achieve the performance of distillation. Second, while distillation strategies are both economical and effective, advancing beyond the boundaries of intelligence may still require more powerful base models and larger-scale reinforcement learning."
    },
    {
        "section": "4_2",
        "content": "\\subsection{Unsuccessful Attempts}\nIn the early stages of developing \\dsri{}, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.\n\n\\paragraph{Process Reward Model (PRM)}\nPRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks~\\citep{uesato2022solving, lightman2023let,mathshepherd}. However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning.\nSecond, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up.\nThird, once a model-based PRM is introduced, it inevitably leads to reward hacking~\\citep{gao2022scalinglawsrewardmodel},  and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search~\\citep{snell2024scalingllmtesttimecompute}, its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.\n\n\\paragraph{Monte Carlo Tree Search (MCTS)}\nInspired by AlphaGo~\\citep{alphago} and AlphaZero~\\citep{alphazero}, we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process.\n\nHowever, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process. Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve. While AlphaGo's core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation.\n\nIn conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge.",
        "trans_content": "\\subsection{Unsuccessful Attempts}\nIn the early stages of developing \\dsri{}, we also encountered failures and setbacks along the way. We share our failure experiences here to provide insights, but this does not imply that these approaches are incapable of developing effective reasoning models.\n\n\\paragraph{Process Reward Model (PRM)}\nPRM is a reasonable method to guide the model toward better approaches for solving reasoning tasks~\\citep{uesato2022solving, lightman2023let,mathshepherd}. However, in practice, PRM has three main limitations that may hinder its ultimate success. First, it is challenging to explicitly define a fine-grain step in general reasoning.\nSecond, determining whether the current intermediate step is correct is a challenging task. Automated annotation using models may not yield satisfactory results, while manual annotation is not conducive to scaling up.\nThird, once a model-based PRM is introduced, it inevitably leads to reward hacking~\\citep{gao2022scalinglawsrewardmodel},  and retraining the reward model needs additional training resources and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good ability to rerank the top-N responses generated by the model or assist in guided search~\\citep{snell2024scalingllmtesttimecompute}, its advantages are limited compared to the additional computational overhead it introduces during the large-scale reinforcement learning process in our experiments.\n\n\\paragraph{Monte Carlo Tree Search (MCTS)}\nInspired by AlphaGo~\\citep{alphago} and AlphaZero~\\citep{alphazero}, we explored using Monte Carlo Tree Search (MCTS) to enhance test-time compute scalability. This approach involves breaking answers into smaller parts to allow the model to explore the solution space systematically. To facilitate this, we prompt the model to generate multiple tags that correspond to specific reasoning steps necessary for the search. For training, we first use collected prompts to find answers via MCTS guided by a pre-trained value model. Subsequently, we use the resulting question-answer pairs to train both the actor model and the value model, iteratively refining the process.\n\nHowever, this approach encounters several challenges when scaling up the training. First, unlike chess, where the search space is relatively well-defined, token generation presents an exponentially larger search space. To address this, we set a maximum extension limit for each node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process. Training a fine-grained value model is inherently difficult, which makes it challenging for the model to iteratively improve. While AlphaGo's core success relied on training a value model to progressively enhance its performance, this principle proves difficult to replicate in our setup due to the complexities of token generation.\n\nIn conclusion, while MCTS can improve performance during inference when paired with a pre-trained value model, iteratively boosting model performance through self-search remains a significant challenge."
    },
    {
        "section": "5",
        "content": "\\section{Conclusion, Limitations, and Future Work}\n\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. \\dsro{} represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. \\dsri{} is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, \\dsri{} achieves performance comparable to OpenAI-o1-1217 on a range of tasks.\n\nWe further explore distillation the reasoning capability to small dense models. We use \\dsri{} as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9\\% on AIME and 83.9\\% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints.\n\nIn the future, we plan to invest in research across the following directions for \\dsri{}.\n<PLACEHOLDER_ENV_23>\n\n\\bibliography{main}\n\n\\newpage\n\\appendix",
        "trans_content": "\\section{Conclusion, Limitations, and Future Work}\n\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement learning. \\dsro{} represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. \\dsri{} is more powerful, leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, \\dsri{} achieves performance comparable to OpenAI-o1-1217 on a range of tasks.\n\nWe further explore distillation the reasoning capability to small dense models. We use \\dsri{} as the teacher model to generate 800K training samples, and fine-tune several small dense models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and Claude-3.5-Sonnet on math benchmarks with 28.9\\% on AIME and 83.9\\% on MATH. Other dense models also achieve impressive results, significantly outperforming other instruction-tuned models based on the same underlying checkpoints.\n\nIn the future, we plan to invest in research across the following directions for \\dsri{}.\n<PLACEHOLDER_ENV_23>\n\n\\bibliography{main}\n\n\\newpage\n\\appendix"
    },
    {
        "section": "6+7",
        "content": "\\section*{Appendix}\n\n\n\\section{Contributions and Acknowledgments}\n\n\\definecolor{damaiblue}{RGB}{0, 0, 100}\n\\definecolor{damaigreen}{RGB}{0, 100, 0}\n\\definecolor{damaired}{RGB}{100, 0, 0}\n\n\\begin{multicols}{2}\n\\noindent\n\\textbf{\\color{damaired} Core Contributors} \\\\\n\\color{damaired} Daya Guo \\\\\n\\color{damaired} Dejian Yang \\\\\n\\color{damaired} Haowei Zhang \\\\\n\\color{damaired} Junxiao Song \\\\\n\\color{damaired} Ruoyu Zhang \\\\\n\\color{damaired} Runxin Xu \\\\\n\\color{damaired} Qihao Zhu \\\\\n\\color{damaired} Shirong Ma \\\\\n\\color{damaired} Peiyi Wang \\\\\n\\color{damaired} Xiao Bi \\\\\n\\color{damaired} Xiaokang Zhang \\\\\n\\color{damaired} Xingkai Yu \\\\\n\\color{damaired} Yu Wu \\\\\n\\color{damaired} Z.F. Wu \\\\\n\\color{damaired} Zhibin Gou \\\\\n\\color{damaired} Zhihong Shao \\\\\n\\color{damaired} Zhuoshu Li \\\\\n\\color{damaired} Ziyi Gao \\\\\n\n\\noindent\n\\textbf{\\color{damaiblue} Contributors} \\\\\n\\color{damaiblue}\n\\color{damaiblue} Aixin Liu \\\\\n\\color{damaiblue} Bing Xue \\\\\n\\color{damaiblue} Bingxuan Wang \\\\\n\\color{damaiblue} Bochao Wu \\\\\n\\color{damaiblue} Bei Feng \\\\\n\\color{damaiblue} Chengda Lu \\\\\n\\color{damaiblue} Chenggang Zhao \\\\\n\\color{damaiblue} Chengqi Deng \\\\\n\\color{damaiblue} Chong Ruan \\\\\n\\color{damaiblue} Damai Dai \\\\\n\\color{damaiblue} Deli Chen \\\\\n\\color{damaiblue} Dongjie Ji \\\\\n\\color{damaiblue} Erhang Li \\\\\n\\color{damaiblue} Fangyun Lin \\\\\n\\color{damaiblue} Fucong Dai \\\\\n\\color{damaiblue} Fuli Luo* \\\\\n\\color{damaiblue} Guangbo Hao \\\\\n\\color{damaiblue} Guanting Chen \\\\\n\\color{damaiblue} Guowei Li \\\\\n\\color{damaiblue} H. Zhang \\\\\n\\color{damaiblue} Hanwei Xu \\\\\n\\color{damaiblue} Honghui Ding \\\\\n\\color{damaiblue} Huazuo Gao \\\\\n\\color{damaiblue} Hui Qu \\\\\n\\color{damaiblue} Hui Li \\\\\n\\color{damaiblue} Jianzhong Guo \\\\\n\\color{damaiblue} Jiashi Li \\\\\n\\color{damaiblue} Jingchang Chen \\\\\n\\color{damaiblue} Jingyang Yuan \\\\\n\\color{damaiblue} Jinhao Tu \\\\\n\\color{damaiblue} Junjie Qiu \\\\\n\\color{damaiblue} Junlong Li \\\\\n\\color{damaiblue} J.L. Cai \\\\\n\\color{damaiblue} Jiaqi Ni \\\\\n\\color{damaiblue} Jian Liang \\\\\n\\color{damaiblue} Jin Chen \\\\\n\\color{damaiblue} Kai Dong \\\\\n\\color{damaiblue} Kai Hu* \\\\\n\\color{damaiblue} Kaichao You \\\\\n\\color{damaiblue} Kaige Gao \\\\\n\\color{damaiblue} Kang Guan \\\\\n\\color{damaiblue} Kexin Huang \\\\\n\\color{damaiblue} Kuai Yu \\\\\n\\color{damaiblue} Lean Wang \\\\\n\\color{damaiblue} Lecong Zhang \\\\\n\\color{damaiblue} Liang Zhao \\\\\n\\color{damaiblue} Litong Wang \\\\\n\\color{damaiblue} Liyue Zhang \\\\\n\\color{damaiblue} Lei Xu \\\\\n\\color{damaiblue} Leyi Xia \\\\\n\\color{damaiblue} Mingchuan Zhang \\\\\n\\color{damaiblue} Minghua Zhang \\\\\n\\color{damaiblue} Minghui Tang \\\\\n\\color{damaiblue} Mingxu Zhou \\\\\n\\color{damaiblue} Meng Li \\\\\n\\color{damaiblue} Miaojun Wang \\\\\n\\color{damaiblue} Mingming Li \\\\\n\\color{damaiblue} Ning Tian \\\\\n\\color{damaiblue} Panpan Huang \\\\\n\\color{damaiblue} Peng Zhang \\\\\n\\color{damaiblue} Qiancheng Wang \\\\\n\\color{damaiblue} Qinyu Chen \\\\\n\\color{damaiblue} Qiushi Du \\\\\n\\color{damaiblue} Ruiqi Ge* \\\\\n\\color{damaiblue} Ruisong Zhang \\\\\n\\color{damaiblue} Ruizhe Pan \\\\\n\\color{damaiblue} Runji Wang \\\\\n\\color{damaiblue} R.J. Chen \\\\\n\\color{damaiblue} R.L. Jin \\\\\n\\color{damaiblue} Ruyi Chen \\\\\n\\color{damaiblue} Shanghao Lu \\\\\n\\color{damaiblue} Shangyan Zhou \\\\\n\\color{damaiblue} Shanhuang Chen \\\\\n\\color{damaiblue} Shengfeng Ye \\\\\n\\color{damaiblue} Shiyu Wang \\\\\n\\color{damaiblue} Shuiping Yu \\\\\n\\color{damaiblue} Shunfeng Zhou \\\\\n\\color{damaiblue} Shuting Pan \\\\\n\\color{damaiblue} S.S. Li \\\\\n\\color{damaiblue} Shuang Zhou \\\\\n\\color{damaiblue} Shaoqing Wu \\\\\n\\color{damaiblue} Shengfeng Ye \\\\\n\\color{damaiblue} Tao Yun \\\\\n\\color{damaiblue} Tian Pei \\\\\n\\color{damaiblue} Tianyu Sun \\\\\n\\color{damaiblue} T. Wang \\\\\n\\color{damaiblue} Wangding Zeng \\\\\n\\color{damaiblue} Wen Liu \\\\\n\\color{damaiblue} Wenfeng Liang \\\\\n\\color{damaiblue} Wenjun Gao \\\\\n\\color{damaiblue} Wenqin Yu* \\\\\n\\color{damaiblue} Wentao Zhang \\\\\n\\color{damaiblue} W.L. Xiao \\\\\n\\color{damaiblue} Wei An \\\\\n\\color{damaiblue} Xiaodong Liu \\\\\n\\color{damaiblue} Xiaohan Wang \\\\\n\\color{damaiblue} Xiaokang Chen \\\\\n\\color{damaiblue} Xiaotao Nie \\\\\n\\color{damaiblue} Xin Cheng \\\\\n\\color{damaiblue} Xin Liu \\\\\n\\color{damaiblue} Xin Xie \\\\\n\\color{damaiblue} Xingchao Liu \\\\\n\\color{damaiblue} Xinyu Yang \\\\\n\\color{damaiblue} Xinyuan Li \\\\\n\\color{damaiblue} Xuecheng Su \\\\\n\\color{damaiblue} Xuheng Lin \\\\\n\\color{damaiblue} X.Q. Li \\\\\n\\color{damaiblue} Xiangyue Jin \\\\\n\\color{damaiblue} Xiaojin Shen \\\\\n\\color{damaiblue} Xiaosha Chen \\\\\n\\color{damaiblue} Xiaowen Sun \\\\\n\\color{damaiblue} Xiaoxiang Wang \\\\\n\\color{damaiblue} Xinnan Song \\\\\n\\color{damaiblue} Xinyi Zhou \\\\\n\\color{damaiblue} Xianzu Wang \\\\\n\\color{damaiblue} Xinxia Shan \\\\\n\\color{damaiblue} Y.K. Li \\\\\n\\color{damaiblue} Y.Q. Wang \\\\\n\\color{damaiblue} Y.X. Wei \\\\\n\\color{damaiblue} Yang Zhang \\\\\n\\color{damaiblue} Yanhong Xu \\\\\n\\color{damaiblue} Yao Li \\\\\n\\color{damaiblue} Yao Zhao \\\\\n\\color{damaiblue} Yaofeng Sun \\\\\n\\color{damaiblue} Yaohui Wang \\\\\n\\color{damaiblue} Yi Yu \\\\\n\\color{damaiblue} Yichao Zhang \\\\\n\\color{damaiblue} Yifan Shi \\\\\n\\color{damaiblue} Yiliang Xiong \\\\\n\\color{damaiblue} Ying He \\\\\n\\color{damaiblue} Yishi Piao \\\\\n\\color{damaiblue} Yisong Wang \\\\\n\\color{damaiblue} Yixuan Tan \\\\\n\\color{damaiblue} Yiyang Ma* \\\\\n\\color{damaiblue} Yiyuan Liu \\\\\n\\color{damaiblue} Yongqiang Guo \\\\\n\\color{damaiblue} Yuan Ou \\\\\n\\color{damaiblue} Yuduan Wang \\\\\n\\color{damaiblue} Yue Gong \\\\\n\\color{damaiblue} Yuheng Zou \\\\\n\\color{damaiblue} Yujia He \\\\\n\\color{damaiblue} Yunfan Xiong \\\\\n\\color{damaiblue} Yuxiang Luo \\\\\n\\color{damaiblue} Yuxiang You \\\\\n\\color{damaiblue} Yuxuan Liu \\\\\n\\color{damaiblue} Yuyang Zhou \\\\\n\\color{damaiblue} Y.X. Zhu \\\\\n\\color{damaiblue} Yanping Huang \\\\\n\\color{damaiblue} Yaohui Li \\\\\n\\color{damaiblue} Yi Zheng \\\\\n\\color{damaiblue} Yuchen Zhu \\\\\n\\color{damaiblue} Yunxian Ma \\\\\n\\color{damaiblue} Ying Tang \\\\\n\\color{damaiblue} Yukun Zha \\\\\n\\color{damaiblue} Yuting Yan \\\\\n\\color{damaiblue} Z.Z. Ren \\\\\n\\color{damaiblue} Zehui Ren \\\\\n\\color{damaiblue} Zhangli Sha \\\\\n\\color{damaiblue} Zhe Fu \\\\\n\\color{damaiblue} Zhean Xu \\\\\n\\color{damaiblue} Zhenda Xie \\\\\n\\color{damaiblue} Zhengyan Zhang \\\\\n\\color{damaiblue} Zhewen Hao \\\\\n\\color{damaiblue} Zhicheng Ma \\\\\n\\color{damaiblue} Zhigang Yan \\\\\n\\color{damaiblue} Zhiyu Wu \\\\\n\\color{damaiblue} Zihui Gu \\\\\n\\color{damaiblue} Zijia Zhu \\\\\n\\color{damaiblue} Zijun Liu* \\\\\n\\color{damaiblue} Zilin Li \\\\\n\\color{damaiblue} Ziwei Xie \\\\\n\\color{damaiblue} Ziyang Song \\\\\n\\color{damaiblue} Zizheng Pan \\\\\n\\color{damaiblue} Zhen Huang \\\\\n\\color{damaiblue} Zhipeng Xu \\\\\n\\color{damaiblue} Zhongyu Zhang \\\\\n\\color{damaiblue} Zhen Zhang \\\\\n\n\\end{multicols}\n\nWithin each role, authors are listed alphabetically by the first name.\nNames marked with * denote individuals who have departed from our team.\n\n\\setcounter{figure}{0}\n\\makeatletter\n<PLACEHOLDER_NEWCOMMAND_193>\n\\makeatother\n\n\\setcounter{table}{0}\n\\makeatletter\n<PLACEHOLDER_NEWCOMMAND_194>\n\\makeatother\n\n\\end{document}",
        "trans_content": "\\section*{Appendix}\n\n\n\\section{Contributions and Acknowledgments}\n\n\\definecolor{damaiblue}{RGB}{0, 0, 100}\n\\definecolor{damaigreen}{RGB}{0, 100, 0}\n\\definecolor{damaired}{RGB}{100, 0, 0}\n\n\\begin{multicols}{2}\n\\noindent\n\\textbf{\\color{damaired} Core Contributors} \\\\\n\\color{damaired} Daya Guo \\\\\n\\color{damaired} Dejian Yang \\\\\n\\color{damaired} Haowei Zhang \\\\\n\\color{damaired} Junxiao Song \\\\\n\\color{damaired} Ruoyu Zhang \\\\\n\\color{damaired} Runxin Xu \\\\\n\\color{damaired} Qihao Zhu \\\\\n\\color{damaired} Shirong Ma \\\\\n\\color{damaired} Peiyi Wang \\\\\n\\color{damaired} Xiao Bi \\\\\n\\color{damaired} Xiaokang Zhang \\\\\n\\color{damaired} Xingkai Yu \\\\\n\\color{damaired} Yu Wu \\\\\n\\color{damaired} Z.F. Wu \\\\\n\\color{damaired} Zhibin Gou \\\\\n\\color{damaired} Zhihong Shao \\\\\n\\color{damaired} Zhuoshu Li \\\\\n\\color{damaired} Ziyi Gao \\\\\n\n\\noindent\n\\textbf{\\color{damaiblue} Contributors} \\\\\n\\color{damaiblue}\n\\color{damaiblue} Aixin Liu \\\\\n\\color{damaiblue} Bing Xue \\\\\n\\color{damaiblue} Bingxuan Wang \\\\\n\\color{damaiblue} Bochao Wu \\\\\n\\color{damaiblue} Bei Feng \\\\\n\\color{damaiblue} Chengda Lu \\\\\n\\color{damaiblue} Chenggang Zhao \\\\\n\\color{damaiblue} Chengqi Deng \\\\\n\\color{damaiblue} Chong Ruan \\\\\n\\color{damaiblue} Damai Dai \\\\\n\\color{damaiblue} Deli Chen \\\\\n\\color{damaiblue} Dongjie Ji \\\\\n\\color{damaiblue} Erhang Li \\\\\n\\color{damaiblue} Fangyun Lin \\\\\n\\color{damaiblue} Fucong Dai \\\\\n\\color{damaiblue} Fuli Luo* \\\\\n\\color{damaiblue} Guangbo Hao \\\\\n\\color{damaiblue} Guanting Chen \\\\\n\\color{damaiblue} Guowei Li \\\\\n\\color{damaiblue} H. Zhang \\\\\n\\color{damaiblue} Hanwei Xu \\\\\n\\color{damaiblue} Honghui Ding \\\\\n\\color{damaiblue} Huazuo Gao \\\\\n\\color{damaiblue} Hui Qu \\\\\n\\color{damaiblue} Hui Li \\\\\n\\color{damaiblue} Jianzhong Guo \\\\\n\\color{damaiblue} Jiashi Li \\\\\n\\color{damaiblue} Jingchang Chen \\\\\n\\color{damaiblue} Jingyang Yuan \\\\\n\\color{damaiblue} Jinhao Tu \\\\\n\\color{damaiblue} Junjie Qiu \\\\\n\\color{damaiblue} Junlong Li \\\\\n\\color{damaiblue} J.L. Cai \\\\\n\\color{damaiblue} Jiaqi Ni \\\\\n\\color{damaiblue} Jian Liang \\\\\n\\color{damaiblue} Jin Chen \\\\\n\\color{damaiblue} Kai Dong \\\\\n\\color{damaiblue} Kai Hu* \\\\\n\\color{damaiblue} Kaichao You \\\\\n\\color{damaiblue} Kaige Gao \\\\\n\\color{damaiblue} Kang Guan \\\\\n\\color{damaiblue} Kexin Huang \\\\\n\\color{damaiblue} Kuai Yu \\\\\n\\color{damaiblue} Lean Wang \\\\\n\\color{damaiblue} Lecong Zhang \\\\\n\\color{damaiblue} Liang Zhao \\\\\n\\color{damaiblue} Litong Wang \\\\\n\\color{damaiblue} Liyue Zhang \\\\\n\\color{damaiblue} Lei Xu \\\\\n\\color{damaiblue} Leyi Xia \\\\\n\\color{damaiblue} Mingchuan Zhang \\\\\n\\color{damaiblue} Minghua Zhang \\\\\n\\color{damaiblue} Minghui Tang \\\\\n\\color{damaiblue} Mingxu Zhou \\\\\n\\color{damaiblue} Meng Li \\\\\n\\color{damaiblue} Miaojun Wang \\\\\n\\color{damaiblue} Mingming Li \\\\\n\\color{damaiblue} Ning Tian \\\\\n\\color{damaiblue} Panpan Huang \\\\\n\\color{damaiblue} Peng Zhang \\\\\n\\color{damaiblue} Qiancheng Wang \\\\\n\\color{damaiblue} Qinyu Chen \\\\\n\\color{damaiblue} Qiushi Du \\\\\n\\color{damaiblue} Ruiqi Ge* \\\\\n\\color{damaiblue} Ruisong Zhang \\\\\n\\color{damaiblue} Ruizhe Pan \\\\\n\\color{damaiblue} Runji Wang \\\\\n\\color{damaiblue} R.J. Chen \\\\\n\\color{damaiblue} R.L. Jin \\\\\n\\color{damaiblue} Ruyi Chen \\\\\n\\color{damaiblue} Shanghao Lu \\\\\n\\color{damaiblue} Shangyan Zhou \\\\\n\\color{damaiblue} Shanhuang Chen \\\\\n\\color{damaiblue} Shengfeng Ye \\\\\n\\color{damaiblue} Shiyu Wang \\\\\n\\color{damaiblue} Shuiping Yu \\\\\n\\color{damaiblue} Shunfeng Zhou \\\\\n\\color{damaiblue} Shuting Pan \\\\\n\\color{damaiblue} S.S. Li \\\\\n\\color{damaiblue} Shuang Zhou \\\\\n\\color{damaiblue} Shaoqing Wu \\\\\n\\color{damaiblue} Shengfeng Ye \\\\\n\\color{damaiblue} Tao Yun \\\\\n\\color{damaiblue} Tian Pei \\\\\n\\color{damaiblue} Tianyu Sun \\\\\n\\color{damaiblue} T. Wang \\\\\n\\color{damaiblue} Wangding Zeng \\\\\n\\color{damaiblue} Wen Liu \\\\\n\\color{damaiblue} Wenfeng Liang \\\\\n\\color{damaiblue} Wenjun Gao \\\\\n\\color{damaiblue} Wenqin Yu* \\\\\n\\color{damaiblue} Wentao Zhang \\\\\n\\color{damaiblue} W.L. Xiao \\\\\n\\color{damaiblue} Wei An \\\\\n\\color{damaiblue} Xiaodong Liu \\\\\n\\color{damaiblue} Xiaohan Wang \\\\\n\\color{damaiblue} Xiaokang Chen \\\\\n\\color{damaiblue} Xiaotao Nie \\\\\n\\color{damaiblue} Xin Cheng \\\\\n\\color{damaiblue} Xin Liu \\\\\n\\color{damaiblue} Xin Xie \\\\\n\\color{damaiblue} Xingchao Liu \\\\\n\\color{damaiblue} Xinyu Yang \\\\\n\\color{damaiblue} Xinyuan Li \\\\\n\\color{damaiblue} Xuecheng Su \\\\\n\\color{damaiblue} Xuheng Lin \\\\\n\\color{damaiblue} X.Q. Li \\\\\n\\color{damaiblue} Xiangyue Jin \\\\\n\\color{damaiblue} Xiaojin Shen \\\\\n\\color{damaiblue} Xiaosha Chen \\\\\n\\color{damaiblue} Xiaowen Sun \\\\\n\\color{damaiblue} Xiaoxiang Wang \\\\\n\\color{damaiblue} Xinnan Song \\\\\n\\color{damaiblue} Xinyi Zhou \\\\\n\\color{damaiblue} Xianzu Wang \\\\\n\\color{damaiblue} Xinxia Shan \\\\\n\\color{damaiblue} Y.K. Li \\\\\n\\color{damaiblue} Y.Q. Wang \\\\\n\\color{damaiblue} Y.X. Wei \\\\\n\\color{damaiblue} Yang Zhang \\\\\n\\color{damaiblue} Yanhong Xu \\\\\n\\color{damaiblue} Yao Li \\\\\n\\color{damaiblue} Yao Zhao \\\\\n\\color{damaiblue} Yaofeng Sun \\\\\n\\color{damaiblue} Yaohui Wang \\\\\n\\color{damaiblue} Yi Yu \\\\\n\\color{damaiblue} Yichao Zhang \\\\\n\\color{damaiblue} Yifan Shi \\\\\n\\color{damaiblue} Yiliang Xiong \\\\\n\\color{damaiblue} Ying He \\\\\n\\color{damaiblue} Yishi Piao \\\\\n\\color{damaiblue} Yisong Wang \\\\\n\\color{damaiblue} Yixuan Tan \\\\\n\\color{damaiblue} Yiyang Ma* \\\\\n\\color{damaiblue} Yiyuan Liu \\\\\n\\color{damaiblue} Yongqiang Guo \\\\\n\\color{damaiblue} Yuan Ou \\\\\n\\color{damaiblue} Yuduan Wang \\\\\n\\color{damaiblue} Yue Gong \\\\\n\\color{damaiblue} Yuheng Zou \\\\\n\\color{damaiblue} Yujia He \\\\\n\\color{damaiblue} Yunfan Xiong \\\\\n\\color{damaiblue} Yuxiang Luo \\\\\n\\color{damaiblue} Yuxiang You \\\\\n\\color{damaiblue} Yuxuan Liu \\\\\n\\color{damaiblue} Yuyang Zhou \\\\\n\\color{damaiblue} Y.X. Zhu \\\\\n\\color{damaiblue} Yanping Huang \\\\\n\\color{damaiblue} Yaohui Li \\\\\n\\color{damaiblue} Yi Zheng \\\\\n\\color{damaiblue} Yuchen Zhu \\\\\n\\color{damaiblue} Yunxian Ma \\\\\n\\color{damaiblue} Ying Tang \\\\\n\\color{damaiblue} Yukun Zha \\\\\n\\color{damaiblue} Yuting Yan \\\\\n\\color{damaiblue} Z.Z. Ren \\\\\n\\color{damaiblue} Zehui Ren \\\\\n\\color{damaiblue} Zhangli Sha \\\\\n\\color{damaiblue} Zhe Fu \\\\\n\\color{damaiblue} Zhean Xu \\\\\n\\color{damaiblue} Zhenda Xie \\\\\n\\color{damaiblue} Zhengyan Zhang \\\\\n\\color{damaiblue} Zhewen Hao \\\\\n\\color{damaiblue} Zhicheng Ma \\\\\n\\color{damaiblue} Zhigang Yan \\\\\n\\color{damaiblue} Zhiyu Wu \\\\\n\\color{damaiblue} Zihui Gu \\\\\n\\color{damaiblue} Zijia Zhu \\\\\n\\color{damaiblue} Zijun Liu* \\\\\n\\color{damaiblue} Zilin Li \\\\\n\\color{damaiblue} Ziwei Xie \\\\\n\\color{damaiblue} Ziyang Song \\\\\n\\color{damaiblue} Zizheng Pan \\\\\n\\color{damaiblue} Zhen Huang \\\\\n\\color{damaiblue} Zhipeng Xu \\\\\n\\color{damaiblue} Zhongyu Zhang \\\\\n\\color{damaiblue} Zhen Zhang \\\\\n\n\\end{multicols}\n\nWithin each role, authors are listed alphabetically by the first name.\nNames marked with * denote individuals who have departed from our team.\n\n\\setcounter{figure}{0}\n\\makeatletter\n<PLACEHOLDER_NEWCOMMAND_193>\n\\makeatother\n\n\\setcounter{table}{0}\n\\makeatletter\n<PLACEHOLDER_NEWCOMMAND_194>\n\\makeatother\n\n\\end{document}"
    }
]